{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the first part, we learned how to use a simple ANN to classify MNIST. \n",
    "#Now we are going to expand our knowledge using a Deep Neural Network.\n",
    "\n",
    "#Architecture of our network is:\n",
    "\n",
    "#    (Input) -> [batch_size, 28, 28, 1] >> Apply 32 filter of [5x5]\n",
    "#    (Convolutional layer 1) -> [batch_size, 28, 28, 32]\n",
    "#    (ReLU 1) -> [?, 28, 28, 32]\n",
    "#    (Max pooling 1) -> [?, 14, 14, 32]\n",
    "#    (Convolutional layer 2) -> [?, 14, 14, 64]\n",
    "#    (ReLU 2) -> [?, 14, 14, 64]\n",
    "#    (Max pooling 2) -> [?, 7, 7, 64]\n",
    "#    [fully connected layer 3] -> [1x1024]\n",
    "#    [ReLU 3] -> [1x1024]\n",
    "#    [Drop out] -> [1x1024]\n",
    "#    [fully connected layer 4] -> [1x10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# finish possible remaining session\n",
    "#sess.close()\n",
    "\n",
    "#Start interactive session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-fb2d6f9f6d93>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Ula\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Ula\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Ula\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Ula\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Ula\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "# just run this untill it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "\n",
    "width = 28 # width of the image in pixels \n",
    "height = 28 # height of the image in pixels\n",
    "flat = width * height # number of pixels in one image \n",
    "class_output = 10 # number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output\n",
    "\n",
    "x  = tf.placeholder(tf.float32, shape=[None, flat]) # input - dimension of input # pixels\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, class_output]) # estimated output - dim = # possible classifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting images of the data set to tensors\n",
    "#The input image is a 28 pixels by 28 pixels, 1 channel (grayscale). \n",
    "#In this case, the first dimension is the batch number of the image, and can be of any size (so we set it to -1).\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])  # input\n",
    "x_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Layer 1\n",
    "\n",
    "#Defining kernel weight and bias\n",
    "#We define a kernle here. The Size of the filter/kernel is 5x5; Input channels is 1 (greyscale); \n",
    "#and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. \n",
    "#So, the output of convolution layer would be 28x28x32). \n",
    "#In this step, we create a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)) # width, height, grayscale, # outputs\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolve with weight tensor and add biases.\n",
    "\n",
    "#To creat convolutional layer, we use tf.nn.conv2d. It computes a 2-D convolution given 4-D input and filter tensors.\n",
    "#Inputs:\n",
    "#   - tensor of shape [batch, in_height, in_width, in_channels]. x of shape [batch_size,28 ,28, 1]\n",
    "#   - a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. \n",
    "# W is of size [5, 5, 1, 32]\n",
    "#   - stride which is [1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. \n",
    "# As the input tensor has 4 dimensions: [batch, height, width, channels], \n",
    "# then the convolution operates on a 2D window on the height and width dimensions. \n",
    "# strides determines how much the window shifts by in each of the dimensions. \n",
    "# As the first and last dimensions are related to batch and channels, we set the stride to 1. \n",
    "# But for second and third dimension, we coould set other values, e.g. [1, 2, 2, 1]\n",
    "#Process:\n",
    "#   - Change the filter to a 2-D matrix with shape [5*5*1,32]\n",
    "#   - Extracts image patches from the input tensor to form a virtual tensor of shape [batch, 28, 28, 5*5*1].\n",
    "#   - For each batch, right-multiplies the filter matrix and the image vector.\n",
    "#Output:\n",
    "#   - A Tensor (a 2-D convolution) of size <tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)- Notice: \n",
    "# the output of the first convolution layer is 32 [28x28] images. \n",
    "# Here 32 is considered as volume/depth of the output image.\n",
    "\n",
    "convolve1= tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the ReLU activation Function\n",
    "h_conv1 = tf.nn.relu(convolve1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(?, 14, 14, 32) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the max pooling\n",
    "\n",
    "#max pooling is a form of non-linear down-sampling. \n",
    "#It partitions the input image into a set of rectangles and, and then find the maximum value for that region.\n",
    "\n",
    "#Lets use tf.nn.max_pool function to perform max pooling. \n",
    "#Kernel size: 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)\n",
    "#Strides: dictates the sliding behaviour of the kernel. \n",
    "#In this case it will move 2 pixels everytime, thus not overlapping. \n",
    "#The input is a matix of size 14x14x32, and the output would be a matrix of size 14x14x32.\n",
    "\n",
    "conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #max_pool_2x2\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Layer 2\n",
    "\n",
    "#Weights and Biases of kernels\n",
    "#We apply the convolution again in this layer. Lets look at the second layer kernel:\n",
    "#    Filter/kernel: 5x5 (25 pixels)\n",
    "#    Input channels: 32 (from the 1st Conv layer, we had 32 feature maps)\n",
    "#    64 output feature maps\n",
    "#Notice: here, the input image is [14x14x32], the filter is [5x5x32], we use 64 filters of size [5x5x32], \n",
    "#and the output of the convolutional layer would be 64 covolved image, [14x14x64].\n",
    "#Notice: the convolution result of applying a filter of size [5x5x32] on image of size [14x14x32] \n",
    "#is an image of size [14x14x1], that is, the convolution is functioning on volume.\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #need 64 biases for 64 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolve image with weight tensor and add biases.\n",
    "\n",
    "convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME')+ b_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the ReLU activation Function\n",
    "\n",
    "h_conv2 = tf.nn.relu(convolve2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_1:0' shape=(?, 7, 7, 64) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the max pooling\n",
    "\n",
    "conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #max_pool_2x2\n",
    "conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second layer completed. So, what is the output of the second layer, layer2?\n",
    "#    it is 64 matrices of [7x7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fully Connected Layer\n",
    "#each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, \n",
    "#which make an array of size [3136x1]. We will connect it into another layer of size [1024x1]. \n",
    "#So, the weight between these 2 layers will be [3136x1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(?, 3136) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_matrix = tf.reshape(conv2, [-1, 7*7*64]) # tensorflow reshapes to 1 dim vector\n",
    "layer2_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_8:0' shape=(3136, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# Weights and Biases between layer 2 and 3\n",
    "\n",
    "#Composition of the feature map from the last layer (7x7) multiplied by the number of feature maps (64); \n",
    "#1027 outputs to Softmax layer\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) # need 1024 biases for 1024 outputs\n",
    "print(W_fc1)\n",
    "print(b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_5:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication (applying weights and biases)\n",
    "fcl = tf.matmul(layer2_matrix, W_fc1) + b_fc1\n",
    "print(fcl)\n",
    "\n",
    "# Apply the ReLU activation Function\n",
    "h_fc1 = tf.nn.relu(fcl)\n",
    "h_fc1\n",
    "\n",
    "#third level complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/mul:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout Layer, Optional phase for reducing overfitting\n",
    "\n",
    "#It is a phase where the network \"forget\" some features. \n",
    "#At each training step in a mini-batch, some units get switched off randomly \n",
    "#so that it will not interact with the network. \n",
    "#That is, it weights cannot be updated, nor affect the learning of the other network nodes. \n",
    "#This can be very useful for very large neural networks to prevent overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "layer_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readout Layer (Softmax Layer)\n",
    "\n",
    "#Weights and Biases\n",
    "#In last layer, CNN takes the high-level filtered images and translate them into votes using softmax. \n",
    "#Input channels: 1024 (neurons from the 3rd Layer); 10 output features\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1)) #1024 neurons\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) # 10 possibilities for digits [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix Multiplication (applying weights and biases)\n",
    "fc = tf.matmul(layer_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the Softmax activation Function\n",
    "#softmax allows us to interpret the outputs of fcl4 as probabilities. So, y_conv is a tensor of probablities.\n",
    "y_CNN = tf.nn.softmax(fc)\n",
    "y_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary of the Deep Convolutional Neural Network\n",
    "\n",
    "#0) Input - MNIST dataset\n",
    "#1) Convolutional and Max-Pooling\n",
    "#2) Convolutional and Max-Pooling\n",
    "#3) Fully Connected Layer\n",
    "#4) Processing - Dropout\n",
    "#5) Readout layer - Fully Connected\n",
    "#6) Outputs - Classified digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define functions and train the model\n",
    "#Define the loss function\n",
    "\n",
    "#We need to compare our output, layer4 tensor, with ground truth for all mini_batch. \n",
    "#we can use cross entropy to see how bad our CNN is working - to measure the error at a softmax layer.\n",
    "\n",
    "#The following code shows an toy sample of cross-entropy for a mini-batch of size 2 \n",
    "#which its items have been classified. \n",
    "#You can run it (first change the cell type to code in the toolbar) to see hoe cross entropy changes.\n",
    "\n",
    "#dunno what's going on in this section\n",
    "import numpy as np\n",
    "layer4_test = [[0.9, 0.1, 0.1],[0.9, 0.1, 0.1]]\n",
    "y_test = [[1.0, 0.0, 0.0],[1.0, 0.0, 0.0]]\n",
    "np.mean( -np.sum(y_test * np.log(layer4_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
