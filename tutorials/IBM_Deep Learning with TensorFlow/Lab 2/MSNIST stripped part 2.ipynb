{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the first part, we learned how to use a simple ANN to classify MNIST. \n",
    "#Now we are going to expand our knowledge using a Deep Neural Network.\n",
    "\n",
    "#Architecture of our network is:\n",
    "\n",
    "#    (Input) -> [batch_size, 28, 28, 1] >> Apply 32 filter of [5x5]\n",
    "#    (Convolutional layer 1) -> [batch_size, 28, 28, 32]\n",
    "#    (ReLU 1) -> [?, 28, 28, 32]\n",
    "#    (Max pooling 1) -> [?, 14, 14, 32]\n",
    "#    (Convolutional layer 2) -> [?, 14, 14, 64]\n",
    "#    (ReLU 2) -> [?, 14, 14, 64]\n",
    "#    (Max pooling 2) -> [?, 7, 7, 64]\n",
    "#    [fully connected layer 3] -> [1x1024]\n",
    "#    [ReLU 3] -> [1x1024]\n",
    "#    [Drop out] -> [1x1024]\n",
    "#    [fully connected layer 4] -> [1x10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# finish possible remaining session\n",
    "#sess.close()\n",
    "\n",
    "#Start interactive session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "# just run this untill it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "\n",
    "width = 28 # width of the image in pixels \n",
    "height = 28 # height of the image in pixels\n",
    "flat = width * height # number of pixels in one image \n",
    "class_output = 10 # number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input and output\n",
    "\n",
    "x  = tf.placeholder(tf.float32, shape=[None, flat])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, class_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting images of the data set to tensors\n",
    "#The input image is a 28 pixels by 28 pixels, 1 channel (grayscale). \n",
    "#In this case, the first dimension is the batch number of the image, and can be of any size (so we set it to -1).\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])  \n",
    "x_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolutional Layer 1\n",
    "\n",
    "#Defining kernel weight and bias\n",
    "#We define a kernle here. The Size of the filter/kernel is 5x5; Input channels is 1 (greyscale); \n",
    "#and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. \n",
    "#So, the output of convolution layer would be 28x28x32). \n",
    "#In this step, we create a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # need 32 biases for 32 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolve with weight tensor and add biases.\n",
    "\n",
    "#To creat convolutional layer, we use tf.nn.conv2d. It computes a 2-D convolution given 4-D input and filter tensors.\n",
    "#Inputs:\n",
    "#   - tensor of shape [batch, in_height, in_width, in_channels]. x of shape [batch_size,28 ,28, 1]\n",
    "#   - a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. \n",
    "# W is of size [5, 5, 1, 32]\n",
    "#   - stride which is [1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. \n",
    "# As the input tensor has 4 dimensions: [batch, height, width, channels], \n",
    "# then the convolution operates on a 2D window on the height and width dimensions. \n",
    "# strides determines how much the window shifts by in each of the dimensions. \n",
    "# As the first and last dimensions are related to batch and channels, we set the stride to 1. \n",
    "# But for second and third dimension, we coould set other values, e.g. [1, 2, 2, 1]\n",
    "#Process:\n",
    "#   - Change the filter to a 2-D matrix with shape [5*5*1,32]\n",
    "#   - Extracts image patches from the input tensor to form a virtual tensor of shape [batch, 28, 28, 5*5*1].\n",
    "#   - For each batch, right-multiplies the filter matrix and the image vector.\n",
    "#Output:\n",
    "#   - A Tensor (a 2-D convolution) of size <tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)- Notice: \n",
    "# the output of the first convolution layer is 32 [28x28] images. \n",
    "# Here 32 is considered as volume/depth of the output image.\n",
    "\n",
    "convolve1= tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply the ReLU activation Function\n",
    "h_conv1 = tf.nn.relu(convolve1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(?, 14, 14, 32) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the max pooling\n",
    "\n",
    "#max pooling is a form of non-linear down-sampling. \n",
    "#It partitions the input image into a set of rectangles and, and then find the maximum value for that region.\n",
    "\n",
    "#Lets use tf.nn.max_pool function to perform max pooling. \n",
    "#Kernel size: 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)\n",
    "#Strides: dictates the sliding behaviour of the kernel. \n",
    "#In this case it will move 2 pixels everytime, thus not overlapping. \n",
    "#The input is a matix of size 14x14x32, and the output would be a matrix of size 14x14x32.\n",
    "\n",
    "conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #max_pool_2x2\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolutional Layer 2\n",
    "\n",
    "#Weights and Biases of kernels\n",
    "#We apply the convolution again in this layer. Lets look at the second layer kernel:\n",
    "#    Filter/kernel: 5x5 (25 pixels)\n",
    "#    Input channels: 32 (from the 1st Conv layer, we had 32 feature maps)\n",
    "#    64 output feature maps\n",
    "#Notice: here, the input image is [14x14x32], the filter is [5x5x32], we use 64 filters of size [5x5x32], \n",
    "#and the output of the convolutional layer would be 64 covolved image, [14x14x64].\n",
    "#Notice: the convolution result of applying a filter of size [5x5x32] on image of size [14x14x32] \n",
    "#is an image of size [14x14x1], that is, the convolution is functioning on volume.\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #need 64 biases for 64 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolve image with weight tensor and add biases.\n",
    "\n",
    "convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME')+ b_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply the ReLU activation Function\n",
    "\n",
    "h_conv2 = tf.nn.relu(convolve2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool_2:0' shape=(?, 7, 7, 64) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the max pooling\n",
    "\n",
    "conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') #max_pool_2x2\n",
    "conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second layer completed. So, what is the output of the second layer, layer2?\n",
    "#    it is 64 matrix of [7x7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
