{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I want to print\n",
      "food       221\n",
      "animal      31\n",
      "many        26\n",
      "may         24\n",
      "culture     23\n",
      "price       21\n",
      "include     20\n",
      "cooking     19\n",
      "taste       18\n",
      "meat        18\n",
      "plant       18\n",
      "world       18\n",
      "type        18\n",
      "health      16\n",
      "used        16\n",
      "also        16\n",
      "people      16\n",
      "country     15\n",
      "product     14\n",
      "diet        14\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_lexicon(fileIn, n):\n",
    "    #nltk.download('wordnet')\n",
    "    #nltk.download(\"stopwords\")\n",
    "    #nltk.download(\"punkt\")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    lemmitizer = WordNetLemmatizer()\n",
    "    lexicon=[]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    with open(fileIn, 'r') as f:\n",
    "        # f is the fileobject\n",
    "        # contents is a list of strings where each string is a line\n",
    "        # of the text in the fileobject\n",
    "        contents = f.readlines()\n",
    "  \n",
    "        for l in contents[:]:\n",
    "            # l is the line of text, a string\n",
    "            all_words = word_tokenize(l)\n",
    "            # all words is a list of strings, each string a word from the line\n",
    "            \n",
    "            for i in all_words:\n",
    "                i = i.lower()\n",
    "                if i not in stop_words:\n",
    "                    if i.isalnum():\n",
    "                        lexicon.append(i)\n",
    "    \n",
    "    lexicon = [lemmitizer.lemmatize(i) for i in lexicon]\n",
    "\n",
    "    #print(lexicon)\n",
    "    w_counts = Counter(lexicon)\n",
    "    \n",
    "    l2 = dict(w_counts)\n",
    "    #l2 = {}\n",
    "    #for w in w_counts:\n",
    "    #    l2[w] = w_counts[w]\n",
    "        \n",
    "    l2 = pd.Series(l2)\n",
    "\n",
    "    l2.sort_values(inplace=True, ascending=False)\n",
    "    \n",
    "    return l2.head(n)\n",
    "\n",
    "def main():\n",
    "    print(\"What I want to print\")\n",
    "    scores = create_lexicon(\"script1.txt\", 20)\n",
    "    print(scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
