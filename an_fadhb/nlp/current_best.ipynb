{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from IPython.display import display\n",
    "from itertools import combinations\n",
    "from copy import deepcopy\n",
    "from pprint import pprint as pp\n",
    "\n",
    "#import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', 11)\n",
    "pd.set_option('display.width', 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_definition_from_first_paragraph(text):\n",
    "    \"\"\" \n",
    "    Extract the first paragraph from the text as a string.\n",
    "    It is also removed from the text.\n",
    "    \n",
    "    This will need to be rewritten as a class later so that it\n",
    "    does not have to return the text.\n",
    "    \n",
    "    Actually, after analysing the word counts in each first paragraph,\n",
    "    I found that the most common word in the first paragraph is always\n",
    "    the class definition. The problem is that for transcript2 the class\n",
    "    is a phrase (fast food), not a word. I don't have time to do the \n",
    "    phrase logic right now so I am just going to pass the name of the\n",
    "    class manually.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    first_paragraph = str()\n",
    "    \n",
    "    # iterate through every line of the text\n",
    "    for position, line in enumerate(text):\n",
    "\n",
    "        # if the line contains only space we stop \n",
    "        if not line:                \n",
    "            text = text[position::]\n",
    "            break\n",
    "\n",
    "        # otherwise we pop out the line and add it to the first paragraph\n",
    "        else:\n",
    "            first_paragraph=first_paragraph+line\n",
    "            \n",
    "    # return {class_name: first_paragraph}, text\n",
    "    return first_paragraph, text\n",
    "\n",
    "\n",
    "def extract_feature_definitions_from_remaining_text(text):\n",
    "    important_words = {}\n",
    "        \n",
    "    # iterate through every line of the text\n",
    "    for position, line in enumerate(text):\n",
    "                \n",
    "        # if the previous 2 lines contain only space we have reached a paragraph heading\n",
    "        if not text[position-2] and not text[position-1]:\n",
    "            # drop the space at the start and the end of the heading\n",
    "            heading = line[1:-1]\n",
    "            important_words[heading] = str()\n",
    "\n",
    "        # otherwise we add the line to the first paragraph string\n",
    "        elif line:\n",
    "            important_words[heading] += line\n",
    "        \n",
    "    return important_words\n",
    "\n",
    "\n",
    "def extract_important_words(input_text):\n",
    "    \"\"\"\n",
    "    Given a text in the format of those supplied with the i2x brainteaser, this method: \n",
    "    1. extracts the first paragraph as a general definition of the text's subject (henceforth referred to as the 'class).\n",
    "    2. takes each following paragraph-title as a subclass of the class.  (The subclass is really a feature of the class \n",
    "       but each subclass has words as features as well so we will separate the naming to avoid confusion.)\n",
    "    3. takes each paragraph as a definition of its heading/feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy the text so we do not alter the original\n",
    "    text = input_text.copy()\n",
    "   \n",
    "    first_paragraph, remaining_text = extract_class_definition_from_first_paragraph(text)\n",
    "    important_words = extract_feature_definitions_from_remaining_text(remaining_text)\n",
    "    \n",
    "    # Get rid of the references, etc.\n",
    "    for key in ['References', 'Notes', 'External links', 'Further reading', 'See also']:\n",
    "        if key in important_words.keys():\n",
    "            important_words.pop(key, None)\n",
    "\n",
    "    return first_paragraph, important_words\n",
    "\n",
    "\n",
    "def get_word_counts(paragraph):\n",
    "    lexicon = create_lexicon(paragraph)\n",
    "    word_counts = count_word_occurrences(lexicon)\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def extract_class_definition_from_text(class_name, filepath):\n",
    "    \n",
    "    text = import_text_and_split_on_spaces(filepath)\n",
    "    \n",
    "    class_definition, features = extract_important_words(text)\n",
    "    class_definition = get_word_counts(class_definition)\n",
    "    class_definition.rename(class_name, inplace=True)\n",
    "    \n",
    "    list_of_feature_series = []\n",
    "    for feature, definition in features.items():\n",
    "        feature_series = get_word_counts(definition)\n",
    "        feature_series.rename(feature, inplace=True)\n",
    "        list_of_feature_series.append(pd.DataFrame(feature_series))\n",
    "        \n",
    "    features_df = pd.concat(list_of_feature_series)\n",
    "    \n",
    "    features_df.sort_values('Allergies', inplace=True, ascending=False)\n",
    "    \n",
    "    return class_definition, features_df\n",
    "\n",
    "\n",
    "def import_text_and_split_on_spaces(filepath):\n",
    "    file_object = open(filepath, mode='r')\n",
    "    # Import the text as a string\n",
    "    text = file_object.read()\n",
    "    # Split it into a list where each element is a line in string format\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def count_word_occurrences(lexicon):\n",
    "    \"\"\"\n",
    "    Given a list of words, this method returns a series with the frequency of each word within the list\n",
    "    :param lexicon: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    w_counts = Counter(lexicon)\n",
    "    \n",
    "    l2 = dict(w_counts)\n",
    "        \n",
    "    l2 = pd.Series(l2)\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    l2 = l2/l2.sum()\n",
    "    \n",
    "    # Sort the values\n",
    "    l2.sort_values(inplace=True, ascending=False)\n",
    "        \n",
    "    return l2\n",
    "\n",
    "\n",
    "def create_lexicon(paragraph):\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmitizer = WordNetLemmatizer()\n",
    "    # tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    lexicon=[]\n",
    "    \n",
    "    all_words = word_tokenize(paragraph)\n",
    "    \n",
    "    for i in all_words:\n",
    "        i = i.lower()\n",
    "        if i not in stop_words:\n",
    "            if i.isalnum():\n",
    "                lexicon.append(i)\n",
    "\n",
    "    lexicon = [lemmitizer.lemmatize(i) for i in lexicon]\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def create_lexicon_from_full_text(file_name, n):\n",
    "    \"\"\"\n",
    "    Given a text this method creates a lexicon and scores the words in the lexicon by their frequency\n",
    "    \"\"\"\n",
    "    text = import_text_and_split_on_spaces(file_name)\n",
    "    lexicon = []\n",
    "    for line in text:\n",
    "        lexicon.extend(create_lexicon(line))    \n",
    "    \n",
    "    l2 = count_word_occurrences(lexicon)\n",
    "    \n",
    "    # w_counts = Counter(lexicon)\n",
    "    # l2 = dict(w_counts)\n",
    "    # l2 = pd.Series(l2)\n",
    "    # l2 = l2/l2.sum()\n",
    "    # l2.sort_values(inplace=True, ascending=False)\n",
    "    \n",
    "    return l2.head(n)\n",
    "\n",
    "\n",
    "def compare_text_similarity(definition, comparison_definition):\n",
    "    \"\"\"\n",
    "    This method uses a modified Naive Bayes classifier to compare the word vectors\n",
    "    \n",
    "    :param definition: \n",
    "    :param comparison_definition: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    prob = definition.multiply(comparison_definition)\n",
    "    \n",
    "    prob.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "    score = prob.sum()/definition.size\n",
    "    \n",
    "    return definition.name, score\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    training_text_filepath = 'script1.txt'\n",
    "    class_name = 'food'\n",
    "    \n",
    "    comparison_texts = ['transcript_1.txt', 'transcript_2.txt', 'transcript_3.txt']\n",
    "    \n",
    "    for comparison_text_filepath in comparison_texts:\n",
    "        \n",
    "        class_definition, class_features = extract_class_definition_from_text(class_name, training_text_filepath)\n",
    "\n",
    "        text_definition = create_lexicon_from_full_text(comparison_text_filepath, 100)\n",
    "\n",
    "        class_name, main_score = compare_text_similarity(class_definition, text_definition)\n",
    "\n",
    "        results = {}\n",
    "        for col in class_features.columns:\n",
    "            sub_class, score = compare_text_similarity(class_features[col], text_definition)\n",
    "            results[sub_class] = score\n",
    "\n",
    "        print('The general similarity score for ' + comparison_text_filepath + ' with the class ' + class_name + ' is:', main_score)\n",
    "        results = pd.Series(results)\n",
    "        results.sort_values(inplace=True, ascending=False)\n",
    "        print('\\nWith respect to this definition, ' + comparison_text_filepath + ' is about:')\n",
    "        print(results)\n",
    "        print('\\n-----------------------------------------------------------------------------\\n')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}