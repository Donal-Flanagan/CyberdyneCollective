{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_lexicon(fileIn, n):\n",
    "    #nltk.download('wordnet')\n",
    "    #nltk.download(\"stopwords\")\n",
    "    #nltk.download(\"punkt\")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    lemmitizer = WordNetLemmatizer()\n",
    "    lexicon=[]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    with open(fileIn, 'r') as f:\n",
    "        # f is the fileobject\n",
    "        # contents is a list of strings where each string is a line\n",
    "        # of the text in the fileobject\n",
    "        contents = f.readlines()\n",
    "  \n",
    "        for l in contents[:]:\n",
    "            # l is the line of text, a string\n",
    "            all_words = word_tokenize(l)\n",
    "            # all words is a list of strings, each string a word from the line\n",
    "            \n",
    "            for i in all_words:\n",
    "                i = i.lower()\n",
    "                print(i)\n",
    "                if i not in stop_words:\n",
    "                    print(i)\n",
    "                    if i.isalnum():\n",
    "                        print(i)\n",
    "                        lexicon.append(i)\n",
    "    \n",
    "    lexicon = [lemmitizer.lemmatize(i) for i in lexicon]\n",
    "\n",
    "    #print(lexicon)\n",
    "    w_counts = Counter(lexicon)\n",
    "    \n",
    "    l2 = dict(w_counts)\n",
    "    #l2 = {}\n",
    "    #for w in w_counts:\n",
    "    #    l2[w] = w_counts[w]\n",
    "        \n",
    "    l2 = pd.Series(l2)\n",
    "\n",
    "    l2.sort_values(inplace=True, ascending=False)\n",
    "    \n",
    "    return l2.head(n)\n",
    "\n",
    "def main():\n",
    "    print(\"What I want to print\")\n",
    "    scores = create_lexicon(\"script1.txt\", 20)\n",
    "    print(scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    script1 = create_lexicon(\"script1.txt\", 200)\n",
    "    transcript1 = create_lexicon(\"transcript_1.txt\", 200)\n",
    "    transcript2 = create_lexicon(\"transcript_2.txt\", 200)\n",
    "    transcript3 = create_lexicon(\"transcript_3.txt\", 200)\n",
    "    \n",
    "    scores = pd.DataFrame()\n",
    "    \n",
    "    scores['script1'] = script1\n",
    "    scores['transcript1'] = transcript1\n",
    "    scores['transcript2'] = transcript2\n",
    "    scores['transcript3'] = transcript3\n",
    "    scores.sort_values('transcript3', inplace=True, ascending=False)\n",
    "    print(scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
