{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sci-Hub server log data analysis*\n",
    "\n",
    "### *John Bohannon, Science magazine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will help you process the 6 months of raw server log data provided by Sci-Hub to Science magazine in March 2016.\n",
    "\n",
    "**Science article:**\n",
    "\n",
    "http://www.sciencemag.org/news/2016/04/whos-downloading-pirated-papers-everyone\n",
    "\n",
    "**Data set:**\n",
    "\n",
    "http://dx.doi.org/10.5061/dryad.q447c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = (\"sep2015\", \"oct2015\", \"nov2015\", \"dec2015\", \"jan2016\", \"feb2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir \"scihub_data_temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will need CrossRef's database of DOI prefixes to identify publishers from the DOIs.**\n",
    "\n",
    "http://www.crossref.org/06members/50go-live.html\n",
    "\n",
    "**Don't worry, I scraped all that for you:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_DOIs = pd.read_csv(\"publisher_DOI_prefixes.csv\", index_col = 0)\n",
    "journal_DOIs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holding all months of raw data in memory is a bit much for most laptop computers, so let's process each month separately to generate aggregate data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(month):\n",
    "    \n",
    "    # load the file as a dataframe\n",
    "    path = \"scihub_data/\"\n",
    "    filename = month + \".tab\"\n",
    "    with open(path + filename, \"r\") as f:\n",
    "        data = pd.read_table(f)\n",
    "\n",
    "    # this is the format of the columns\n",
    "    data.columns = [\"date\",\"doi\",\"IP_code\",\"country\",\"city\",\"coords\"]\n",
    "\n",
    "    # create a few more useful columns\n",
    "    data[[\"latitude\", \"longitude\"]] = data.coords.str.split(\",\", expand = True)\n",
    "    data[\"prefix\"] = data.dropna(subset = [\"doi\"]).doi.apply(lambda x: x.split(\"/\")[0])\n",
    "\n",
    "    # group by DOI prefix and count total downloads for each\n",
    "    publishers = data.dropna(subset = [\"prefix\"]).groupby(\"prefix\").count()\n",
    "    publishers = publishers.sort_values(by = \"date\", ascending = False).date\n",
    "    publishers = publishers.reset_index()\n",
    "    publishers.columns = [\"prefix\",\"downloads\"]\n",
    "\n",
    "    # translate those DOI prefixes into publisher names using the CrossRef data\n",
    "    data_publishers = pd.merge(publishers, journal_DOIs[[\"Prefix\",\"Name\"]],\n",
    "                               left_on = \"prefix\", right_on = \"Prefix\", how = \"left\")\n",
    "    data_publishers[[\"prefix\",\"downloads\",\"Name\"]].to_csv(\"scihub_data_temp/%s_publishers.csv\" %(month))\n",
    "\n",
    "    # calculate the 100 most downloaded DOIs of the month\n",
    "    top100_doi = data.groupby(\"doi\").count().sort_values(by = \"date\", ascending = False)[:100].date\n",
    "    top100_doi.name = \"downloads\"\n",
    "    top100_doi.to_csv(\"scihub_data_temp/%s_top100_doi.csv\" %month, header = \"downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    print(month)\n",
    "    process_data(month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's see the big picture, starting with the publishers...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers_by_month = [pd.read_csv(\"scihub_data_temp/\" + i + \"_publishers.csv\") for i in months]\n",
    "for i in publishers_by_month:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_publishers = pd.concat(publishers_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_publishers = all_publishers.groupby(\"Name\").sum().downloads.sort_values(ascending = False)\n",
    "all_publishers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([int(i[i.Name == \"Elsevier\"].downloads) for i in publishers_by_month])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yep, that checks out. Nearly 10 million Elsevier downloads in 6 months.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_publishers.to_csv(\"downloads_by_publishers.csv\", header = [\"downloads\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next let's get a list of most downloaded papers across the 6-month period.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = pd.concat([pd.read_csv(\"scihub_data_temp/\" + i + \"_top100_doi.csv\") for i in months])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers.groupby(\"doi\").count().sort_values(by = \"downloads\",ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plenty of papers are in the top100 across all 6 months. Let's see what the most downloaded paper is across the entire time period...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25_doi = all_papers.groupby(\"doi\").sum().downloads.sort_values(ascending = False)[:25]\n",
    "top25_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list()\n",
    "for month in months:\n",
    "    print(month)\n",
    "    datafile = \"scihub_data/\" + month + \".tab\"\n",
    "    with open(datafile, \"r\") as f:\n",
    "        this_month = pd.read_table(f)\n",
    "        this_month.columns = [\"date\", \"doi\", \"IP_code\", \"country\", \"city\", \"coords\"]\n",
    "        data_list.append(this_month.groupby(\"doi\").count().date)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_list[0]\n",
    "for i in data_list[1:]:\n",
    "    data = data.add(i, fill_value = 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name = \"downloads\"\n",
    "data = data.sort_values(ascending = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that some DOIs are invalid, due to typos from the Sci-Hub users or, in the case of 10.1182/asheducation-2015.1.8, because a website listed the wrong DOI.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100].astype(int).to_csv(\"top100_downloads_by_DOI.csv\", header = \"downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def get_top25_metadata(doi):\n",
    "    fields = [\"title\", \"type\", \"publisher\",\"container-title\", \"subject\", \"published-print.date-parts\"]\n",
    "    metadata = dict([(i, None) for i in fields])\n",
    "    metadata[\"doi\"] = doi\n",
    "    try:\n",
    "        url = \"http://dx.doi.org/\" + doi\n",
    "        headers = {\"accept\": \"application/citeproc+json\"}\n",
    "        r = requests.get(url, headers = headers)\n",
    "        full_metadata = json_normalize(json.loads(r.text))\n",
    "        for i in fields:\n",
    "            if i in full_metadata.columns:\n",
    "                metadata[i] = full_metadata.iloc[0][i]\n",
    "    except:\n",
    "        pass\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top25_metadata(\"10.1007/978-1-4419-9716-6_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25 = pd.DataFrame([get_top25_metadata(i) for i in data[:25].index.values])\n",
    "top25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25.to_csv(\"top25_papers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's look at the geography of Sci-Hub downloads.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_data(month):\n",
    "    path = \"scihub_data/\"\n",
    "    filename = month + \".tab\"\n",
    "    with open(path + filename, \"r\") as f:\n",
    "        data = pd.read_table(f)\n",
    "        data.columns = [\"date\", \"doi\", \"IP_code\", \"country\", \"city\", \"coords\"]\n",
    "        data = data[[\"country\", \"date\"]]\n",
    "        data = data.groupby(\"country\").count()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [get_country_data(month) for month in months]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country = data_list[0]\n",
    "for i in data_list[1:]:\n",
    "    by_country = by_country.add(i)\n",
    "data_list = None\n",
    "by_country = by_country.sort_values(by = \"date\", ascending = False)\n",
    "by_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country.columns = [\"downloads\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_country.to_csv(\"downloads_by_country.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to get downloads by lat/lon coordinates...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cities(month):\n",
    "    filepath = \"scihub_data/\" + month + \".tab\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = pd.read_table(f)\n",
    "        data.columns = [\"date\",\"doi\",\"IP_code\",\"country\",\"city\",\"coords\"]\n",
    "        data = data.groupby([\"coords\",\"city\",\"country\"]).count()\n",
    "        data = data.rename(columns = {\"date\":\"downloads\"})\n",
    "        return data[\"downloads\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads_by_coords = get_cities(months[0])\n",
    "for month in months[1:]:\n",
    "    print(month)\n",
    "    downloads_by_coords = downloads_by_coords.add(get_cities(month), fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(downloads_by_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So there are about 22,000 locations with unique triples of (lat/lon, city name, country name).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads_by_coords = downloads_by_coords.reset_index()\n",
    "downloads_by_coords.columns = [\"coords\",\"city\",\"country\",\"downloads\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads_by_coords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some coords cluster to the same city/country. Let's find them...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_coords = downloads_by_coords.reset_index()\n",
    "coord_dupes = by_coords[by_coords.duplicated(subset=[\"city\",\"country\"])]\n",
    "coord_dupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeah look at that. Many Sci-Hub user IP addresses clustered to different lat/lon coordinates within the same city, probably because the Google Maps API treats big cities as supersets of several smaller cities. We'll need to pull out the lower-level \"administrative_area_level\" names using Google Maps...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From here to the end of this Notebook is data-wrangling I needed to build the map that features in the Science article. You probably don't need any of this for your own analyses. The code above gets you to the starting point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# You will need to register with Google Maps and get your own free API key\n",
    "# https://developers.google.com/maps/documentation/javascript/get-api-key\n",
    "API_KEY = \"AjZaSyCaecKeKEr9NEv4zXaPzVVSds1FLTrtM4x\"\n",
    "\n",
    "def get_admin(coords, country):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?key=%s&latlng=%s\" %(API_KEY, coords)\n",
    "    r = requests.get(url)\n",
    "    data = json_normalize(json.loads(r.text)[\"results\"], \"address_components\")\n",
    "    if country == \"United States\":\n",
    "        level = \"administrative_area_level_1\"\n",
    "    else:\n",
    "        level = \"administrative_area_level_4\"\n",
    "    return data[data.types.map(lambda x: level in x)][\"short_name\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_admin(\"-15.813415,-48.1044183\", \"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"downloads_by_coords.csv\") as f:\n",
    "    by_coords = pd.read_csv(f)\n",
    "by_coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous = by_coords[by_coords.duplicated([\"city\",\"country\"], keep = False)].copy()\n",
    "ambiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So we have 2106 city names duplicated within countries...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous[ambiguous.city == \"Sterling\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the US, we want the state, which is administrative_area_level_1 in the Google Maps JSON.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_admin(\"39.0026518,-77.3956004\", \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_cities = ambiguous[ambiguous.country == \"United States\"].copy()\n",
    "US_coords = US_cities[\"coords\"].tolist()\n",
    "get_admin(US_coords[0], \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_states = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "total = len(US_coords)\n",
    "fails = list()\n",
    "for n,i in enumerate(US_coords):\n",
    "    sys.stdout.write(\"%s\\t%s\" %(total - n, len(fails)))\n",
    "    if i not in US_states:\n",
    "        try:\n",
    "            US_states[i] = get_admin(i, \"United States\")\n",
    "        except:\n",
    "            fails.append(i)\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write('\\r')\n",
    "print(\"%s\\t%s\" %(total - n, len(fails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(US_states.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That takes care of the US states. Now to deal with non-US cities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous[\"admin\"] = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_US_cities = ambiguous[ambiguous.country != \"United States\"].copy()\n",
    "non_US_coords = non_US_cities[\"coords\"].tolist()\n",
    "len(non_US_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not too bad. Just 363 non-US locations to deal with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# You will need to register with Google Maps and get your own free API key\n",
    "# https://developers.google.com/maps/documentation/javascript/get-api-key\n",
    "API_KEY = \"AjZaSyCaecKeKEr9NEv4zXaPzVVSds1FLTrtM4x\"\n",
    "\n",
    "def get_non_US_admin(coords):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?key=%s&latlng=%s\" %(API_KEY, coords)\n",
    "    r = requests.get(url)\n",
    "    data = json_normalize(json.loads(r.text)[\"results\"], \"address_components\")\n",
    "    try:\n",
    "        level = \"administrative_area_level_4\"\n",
    "        return data[data.types.map(lambda x: level in x)][\"short_name\"].iloc[0]\n",
    "    except:\n",
    "        try:\n",
    "            level = \"administrative_area_level_3\"\n",
    "            return data[data.types.map(lambda x: level in x)][\"short_name\"].iloc[0]\n",
    "        except:\n",
    "            try:\n",
    "                level = \"administrative_area_level_2\"\n",
    "                return data[data.types.map(lambda x: level in x)][\"short_name\"].iloc[0]\n",
    "            except:\n",
    "                level = \"locality\"\n",
    "                return data[data.types.map(lambda x: level in x)][\"short_name\"].iloc[0]\n",
    "\n",
    "get_non_US_admin(\"65.0120888,25.4650773\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_US_cities = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I started running into the 2500 queries/day limit, requiring multiple fresh API keys. So I'm keeping track of that with error messaging below. Loading the results into a dict as we go allows this to fail gracefully and always pick up where it left off.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "total = len(non_US_coords)\n",
    "fails = list()\n",
    "for n,i in enumerate(non_US_coords):\n",
    "    time.sleep(0.15)\n",
    "    sys.stdout.write(\"%s\\t%s\" %(total - n, len(fails)))\n",
    "    if i not in non_US_cities:\n",
    "        try:\n",
    "            result = get_non_US_admin(i)\n",
    "            if result == \"OVER_QUERY_LIMIT\":\n",
    "                print(\"OVER_QUERY_LIMIT\")\n",
    "                break\n",
    "            else:\n",
    "                non_US_cities[i] = result\n",
    "        except:\n",
    "            fails.append(i)\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write('\\r')\n",
    "print(\"%s\\t%s\" %(total - n, len(fails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_and_areas = US_states.copy()\n",
    "for k,v in non_US_cities.items():\n",
    "    coords_and_areas[k] = v\n",
    "len(coords_and_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = \"45.83316,5.096755\"\n",
    "url = \"https://maps.googleapis.com/maps/api/geocode/json?key=%s&latlng=%s\" %(API_KEY, coords)\n",
    "r = requests.get(url)\n",
    "data = json_normalize(json.loads(r.text)[\"results\"], \"address_components\")\n",
    "data[data.types.map(lambda x: \"locality\" in x)][\"short_name\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = pd.Series(non_US_cities)[pd.Series(non_US_cities).duplicated(keep=False)].sort_values()\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locality(coords):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?key=%s&latlng=%s\" %(API_KEY, coords)\n",
    "    r = requests.get(url)\n",
    "    data = json_normalize(json.loads(r.text)[\"results\"], \"address_components\")\n",
    "    return data[data.types.map(lambda x: \"locality\" in x)][\"short_name\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_locality(dupes.index[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localities = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = list()\n",
    "for coords in dupes.index.tolist():\n",
    "    time.sleep(0.15)\n",
    "    localities[coords] = get_locality(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_and_areas.update(localities)\n",
    "len(coords_and_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [i for i in ambiguous.coords if i not in coords_and_areas]\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_areas = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in missing:\n",
    "    time.sleep(0.15)\n",
    "    missing_areas[i] = get_non_US_admin(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_and_areas.update(missing_areas)\n",
    "len(coords_and_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_and_areas = pd.Series(coords_and_areas, name = \"coords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguated_cities = ambiguous.join(coords_and_areas, on=\"coords\", how=\"left\", rsuffix=\"_new\").sort_values(by=\"city\")\n",
    "disambiguated_cities = disambiguated_cities.rename(columns = {\"coords_new\":\"admin_area\"})\n",
    "disambiguated_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguated_cities.groupby([\"coords\",\"country\",\"city\",\"admin_area\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK! All 2106 ambiguous cities are now accounted for.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguated_cities.to_csv(\"disambiguated_cities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's put it all together for mapping...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguated_cities = pd.read_csv(\"disambiguated_cities.csv\", index_col = 0)\n",
    "disambiguated_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_coords = by_coords.join(disambiguated_cities.admin_area, how=\"left\")\n",
    "by_coords[by_coords.admin_area.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stragglers = by_coords[by_coords.duplicated(subset = [\"city\", \"country\", \"admin_area\"], \n",
    "                                            keep = False)].sort_values(by = \"city\")\n",
    "stragglers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are still 629 coords that have identical (country, city, admin_area).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "A# You will need to register with Google Maps and get your own free API key\n",
    "# https://developers.google.com/maps/documentation/javascript/get-api-key\n",
    "API_KEY = \"AjZaSyCaecKeKEr9NEv4zXaPzVVSds1FLTrtM4x\"\n",
    "\n",
    "def get_google_maps_data(coords):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?key=%s&latlng=%s\" %(API_KEY, coords)\n",
    "    r = requests.get(url)\n",
    "    data = json_normalize(json.loads(r.text)[\"results\"], \"address_components\")\n",
    "    level_dict = dict([(i, None) for i in (\"level_1\", \"level_2\", \"level_3\", \"level_4\")])\n",
    "    for level in level_dict:\n",
    "        try:\n",
    "            level_dict[level] = data[data.types.map(lambda x: \"administrative_area_\" + level in x)][\"short_name\"].iloc[0]\n",
    "        except:\n",
    "            pass\n",
    "    return pd.Series(level_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_google_maps_data(\"39.9911093,-76.6699169\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_google_maps_data(\"39.9625984,-76.727745\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And this makes it clear that it is not even possible to disambiguate some of these. Google Maps does not have specific administrative level information that we could use to distinguish them by name. Well, how geographically far are these ambiguous cities from their name duplicate coords?...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "# radius of earth (km)\n",
    "R = 6373.0\n",
    "\n",
    "def geo_distance(coords1, coords2):\n",
    "    lat1, lon1 = [radians(float(i)) for i in coords1.split(\",\")]\n",
    "    lat2, lon2 = [radians(float(i)) for i in coords2.split(\",\")]\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_distance(\"39.9911093,-76.6699169\",\"39.9617415,-76.7471062\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even 7 km is far enough to make the map wonky. So, executive decision: I'm just going to give the duplicates generic names such as \"area 1 near York, PA, United States\"... Expediency!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generic_area_name(map_name, n):\n",
    "    return \"area %s near %s\" %(n, map_name)\n",
    "\n",
    "get_generic_area_name(\"York, PA, United States\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = dict()\n",
    "\n",
    "def fix_stragglers(df):\n",
    "    total = len(stragglers)\n",
    "    x, y, z = df.iloc[0][[\"city\",\"admin_area\", \"country\"]]\n",
    "    for n, c in enumerate(df.coords.tolist()):\n",
    "        if z == \"United States\":\n",
    "            name_dict[c] = get_generic_area_name(\"%s, %s, %s\" %(x, y, z), n + 1)\n",
    "        else:\n",
    "            name_dict[c] = get_generic_area_name(\"%s, %s\" %(x, z), n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = (\"United States\", \"Akron\", \"OH\")\n",
    "akron = stragglers[(stragglers.country == x) & (stragglers.city == y) & (stragglers.admin_area == z)]\n",
    "akron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_stragglers(akron)\n",
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stragglers[[\"coords\",\"city\",\"admin_area\", \"country\"]].values:\n",
    "    x = i\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stragglers[[\"city\",\"admin_area\", \"country\"]].values:\n",
    "    x, y, z = i\n",
    "    df = stragglers[(stragglers.city == x) & (stragglers.admin_area == y) & (stragglers.country == z)]\n",
    "    fix_stragglers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_name(coords):\n",
    "    return name_dict[coords]\n",
    "\n",
    "stragglers[\"map_name\"] = stragglers.coords.apply(make_name)\n",
    "stragglers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_name(admin_area, city, country):\n",
    "    if type(admin_area) != str or admin_area == city:\n",
    "        return \"%s, %s\" %(city, country)\n",
    "    elif country == \"United States\":\n",
    "        return \"%s, %s, %s\" %(city, admin_area, country)\n",
    "    else:\n",
    "        return \"%s, %s, %s\" %(admin_area, city, country)\n",
    "\n",
    "by_coords[\"map_name\"] = by_coords.apply(lambda x: get_map_name(x[\"admin_area\"], x[\"city\"], x[\"country\"]), axis=1)\n",
    "by_coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_coords[by_coords.admin_area.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_straggling = by_coords[by_coords.duplicated(subset=[\"country\",\"city\",\"admin_area\"], keep=False)].sort_values(by=\"city\")\n",
    "still_straggling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = dict()\n",
    "\n",
    "def fix_final_stragglers(df):\n",
    "    total = len(stragglers)\n",
    "    x, y, z = df.iloc[0][[\"city\",\"admin_area\", \"country\"]]\n",
    "    for n, c in enumerate(df.coords.tolist()):\n",
    "        if z == \"United States\":\n",
    "            name_dict[c] = get_generic_area_name(\"%s, %s, %s\" %(x, y, z), n + 1)\n",
    "        else:\n",
    "            name_dict[c] = get_generic_area_name(\"%s, %s\" %(x, z), n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in still_straggling[[\"city\",\"admin_area\", \"country\"]].values:\n",
    "    x, y, z = i\n",
    "    df = still_straggling[(still_straggling.city == x) & (still_straggling.admin_area == y) & (still_straggling.country == z)]\n",
    "    fix_stragglers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_name(coords):\n",
    "    return name_dict[coords]\n",
    "\n",
    "still_straggling[\"map_name\"] = still_straggling.coords.apply(make_name)\n",
    "still_straggling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, I noticed that there are still duplicated records for 6 pairs of coords. Must fix these:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = by_coords[by_coords.duplicated(subset = [\"coords\"], keep = False)].sort_values(by = \"coords\")\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dupes = dupes[dupes.duplicated(subset = [\"coords\"], keep = \"last\")]\n",
    "fixed_dupes = fixed_dupes.set_index(\"coords\")\n",
    "other = dupes[dupes.duplicated(subset = [\"coords\"], keep = \"first\")].set_index(\"coords\")\n",
    "fixed_dupes.downloads = fixed_dupes.downloads.add(other.downloads, fill_value = 0)\n",
    "fixed_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_coords = by_coords.drop(dupes.index)\n",
    "len(by_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_coords = by_coords.set_index(\"coords\")\n",
    "by_coords = by_coords.append(fixed_dupes)\n",
    "len(by_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ready for mapping!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, let's do some frequency analysis...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(month):\n",
    "    filepath = \"scihub_data/\" + month + \".tab\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = pd.read_table(f)\n",
    "        data.columns = [\"date\",\"doi\",\"IP_code\",\"country\",\"city\",\"coords\"]\n",
    "        data = data[[\"date\",\"doi\"]]\n",
    "        data.date = pd.to_datetime(data.date).apply(lambda x: \"%s-%s-%s\" %(x.year, x.month, x.day))\n",
    "        data = data.groupby(\"date\").count()\n",
    "        data = data.reset_index()\n",
    "        data = data.rename(columns = {\"doi\":\"downloads\"})\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day = pd.concat([get_days(month) for month in months])\n",
    "by_day = by_day.groupby(\"date\").sum()\n",
    "by_day.to_csv(\"downloads_by_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day = by_day[:164]\n",
    "by_day.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day[\"day\"] = pd.to_datetime(by_day.date).apply(lambda x: x.isoweekday())\n",
    "by_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "by_day.groupby(\"day\").sum().downloads.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TUESDAY is the busiest day for Sci-Hub. Well how about that?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
