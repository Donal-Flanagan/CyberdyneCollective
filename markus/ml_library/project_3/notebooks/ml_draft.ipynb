{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing, feature_extraction, feature_selection, model_selection, metrics\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'kaggle_titanic'\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "current_dir = os.path.abspath('./')\n",
    "project_dir = current_dir[:current_dir.rfind(project)+len(project)+1]\n",
    "sys.path.insert(0, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = project_dir + 'data/raw/train.csv'\n",
    "test_path = project_dir + 'data/raw/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame.from_csv(train_path)\n",
    "test_df = pd.DataFrame.from_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set slightly imbalanced\n",
    "train_df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_df.columns:\n",
    "    print(train_df[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPrep(object):\n",
    "    \n",
    "    def __init__(self, df, x_columns, y_column,\n",
    "                 replace_dict=None, feature_dict=None, *args, **kwargs):\n",
    "        super(MLPrep, self).__init__()\n",
    "        self.df = df.copy()\n",
    "        self.x_columns = x_columns\n",
    "        self.y_column = y_column\n",
    "        self.replace_dict = replace_dict\n",
    "        self.feature_dict = feature_dict\n",
    "        \n",
    "    def replace(self, replace_dict=None):\n",
    "        \"\"\"replace values in columns by function values or static ones\"\"\"\n",
    "        if replace_dict is None:\n",
    "            if self.replace_dict is None:\n",
    "                print('Nothing to do, no dict specified')\n",
    "                pass\n",
    "        else:\n",
    "            self.replace_dict = replace_dict\n",
    "          \n",
    "\n",
    "        for column, replace_item in self.replace_dict.items():\n",
    "            for value, replace in replace_item.items():\n",
    "                if callable(replace):\n",
    "                    replace_val = replace(self.df[column])\n",
    "                else:\n",
    "                    replace_val = replace\n",
    "    \n",
    "                if value is np.nan:\n",
    "                    self.df.loc[self.df[column].isnull(), column] = replace_val\n",
    "                elif callable(value):\n",
    "                    self.df.loc[value(self.df[column]), column] = replace_val\n",
    "                else:\n",
    "                    self.df.loc[self.df[column]==value, column] = replace_val\n",
    "            \n",
    "    def feature(self, feature_dict=None):\n",
    "        \"\"\"call feature building functions on columns\n",
    "        feature_dict is of form:\n",
    "        {'new_col': function}\n",
    "        {'new_col': (function, (args,))}\n",
    "        best way for function with extra arguments:\n",
    "        {'new_col': partial(function, kwarg=val)}\n",
    "        \n",
    "        function must act on whole df\n",
    "        function(df, **kwargs)\n",
    "        \"\"\"\n",
    "        if feature_dict is None:\n",
    "            if self.feature_dict is None:\n",
    "                print('Nothing to do, no dict specified')\n",
    "                pass\n",
    "        else:\n",
    "            self.feature_dict = feature_dict\n",
    "            \n",
    "        for column, func in self.feature_dict.items():\n",
    "            if isinstance(func, tuple):\n",
    "                self.df[column] = func[0](self.df, *func[1])\n",
    "            else:\n",
    "                self.df[column] = func(self.df)\n",
    "            \n",
    "        \n",
    "    def get_X(self, vectorizer=None, sparse=True, return_features=False):\n",
    "        X_dict = self.df[self.x_columns].to_dict(orient='records')\n",
    "        if vectorizer is None:\n",
    "            self.vectorizer_ = feature_extraction.DictVectorizer(sparse=sparse)\n",
    "            X = self.vectorizer_.fit_transform(X_dict)\n",
    "        else:\n",
    "            self.vectorizer_ = vectorizer\n",
    "            X = self.vectorizer_.transform(X_dict)\n",
    "            \n",
    "        self.feature_columns_ = self.vectorizer_.vocabulary_\n",
    "        \n",
    "        if return_features:\n",
    "            return X, self.feature_columns_\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def get_y(self):\n",
    "        y = self.df[self.y_column]\n",
    "        return np.array(y.replace(y.sort_values().unique(), range(len(y.unique()))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Deck']\n",
    "y_column = 'Survived'\n",
    "replace_dict = {'Age': {np.nan: np.mean},\n",
    "                'Fare': {np.nan: np.mean},\n",
    "                'Embarked': {np.nan: 'nan'},\n",
    "                'Cabin': {np.nan: 'nan'}\n",
    "               }\n",
    "\n",
    "def get_deck(df, col):\n",
    "    return df['Cabin'].apply(lambda s: s[:1])\n",
    "\n",
    "from functools import partial\n",
    "feature_dict = {'Deck': partial(get_deck, col='Cabin'),\n",
    "                'none': lambda df: 0}\n",
    "\n",
    "# feature model\n",
    "fm = MLPrep(train_df, x_columns, y_column, replace_dict, feature_dict)\n",
    "\n",
    "fm.replace()\n",
    "fm.feature()\n",
    "\n",
    "X = fm.get_X()\n",
    "y = fm.get_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.feature_columns_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection.chi2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'objective':'binary:logistic',\n",
    "              'max_depth':3,\n",
    "              'learning_rate':0.1,\n",
    "              'n_estimators':60,\n",
    "              'gamma':0,\n",
    "              'max_delta_step':0,\n",
    "              'nthread':-1,\n",
    "              'silent':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb feature importance\n",
    "model.fit(X, y)\n",
    "xgb.plot_importance(model, importance_type='weight')\n",
    "xgb.plot_importance(model, importance_type='cover')\n",
    "xgb.plot_importance(model, importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "pickle.dump(model, open('modelpickle', 'wb'))\n",
    "joblib.dump(model, open('modeljobib', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual cross val\n",
    "cv_split = model_selection.StratifiedKFold(n_splits=4).split(X, y)\n",
    "\n",
    "for train_idx, test_idx in cv_split:\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more compact than above:\n",
    "model_selection.cross_val_score(model, X, y, scoring='accuracy', cv=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search\n",
    "param_grid = {'max_depth': [3,4,5],\n",
    "              'learning_rate': [0.03, 0.04, 0.1],\n",
    "              'n_estimators': [60, 100, 200],\n",
    "              'gamma': [0],\n",
    "              'min_child_weight': [1],\n",
    "              'max_delta_step': [0],\n",
    "              'subsample': [1],\n",
    "              'colsample_bytree': [1],\n",
    "              'colsample_bylevel': [1],\n",
    "              'reg_alpha': [0],\n",
    "              'reg_lambda': [1],\n",
    "              'scale_pos_weight': [1],\n",
    "             }\n",
    "search_model = model_selection.GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, cv=4, verbose=1)\n",
    "search_model.fit(X, y)\n",
    "print(search_model.best_params_)\n",
    "search_model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score for different hyperparameters. actually depends heavily on cv train-test-split. so better average over multiple runs\n",
    "param_name = 'n_estimators'\n",
    "param_range = range(10,250,20)\n",
    "param_scores = model_selection.validation_curve(model, X, y, param_name, param_range, cv=4, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# score on training set\n",
    "plt.plot(param_range, np.mean(param_scores[0], axis=1))\n",
    "#score on test set\n",
    "plt.plot(param_range, np.mean(param_scores[1], axis=1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve, how training improves with increasing samples\n",
    "learning_scores = model_selection.learning_curve(model, X, y, train_sizes=np.linspace(0.1, 1.0, 5), cv=4, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# learning_scores on training set\n",
    "plt.plot(learning_scores[0], np.mean(learning_scores[1], axis=1))\n",
    "# learning_scores on test set\n",
    "plt.plot(learning_scores[0], np.mean(learning_scores[2], axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.permutation_test_score(model, X, y, cv=4, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally: classification\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, thresh = metrics.precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "plt.plot(prec, rec)\n",
    "plt.show()\n",
    "print(metrics.average_precision_score(y_test, y_pred))\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_pred, drop_intermediate=False)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()\n",
    "# AUC\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced metric for binary (even imbalanced), MCC elem [-1,1]\n",
    "metrics.matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree, ensemble, naive_bayes, neighbors, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {'criterion': 'gini',\n",
    "               'max_depth': 4,\n",
    "               'min_samples_split': 2,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(**tree_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
