{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with the Millennium Development Goals challenge\n",
    "\n",
    "-----------\n",
    "\n",
    "### This notebook is available for [viewing](http://nbviewer.ipython.org/gist/pjbull/c3b7a16fe5a690813c09) and [download](https://gist.githubusercontent.com/pjbull/c3b7a16fe5a690813c09/raw/5a92f81bb230742e9bca853222e2653ff641a5a2/World%20Bank%20-%20Getting%20Started.ipynb) NBViewer and as a Github gist.\n",
    "\n",
    "----------\n",
    "\n",
    "People have been a little reticent to start digging in to the [Millenium Development Goals](http://www.drivendata.org/competitions/1/) challenge. We thought a little sample code might help get things started.\n",
    "\n",
    "The UN measures progress towards Millennium Development Goals using macroeconomic indicators such as percent of the population making over one dollar per day. Each goal has one or more targets, and each target has one or more indicators. Of the 60 indicators in all, we've chosen a subset to focus on for this challenge. Your task is to predict the change in these indicators one year and five years into the \"future\" from the year 2007.\n",
    "\n",
    "Predicting future progress will help us to understand how we achieve these goals by uncovering complex relations between these goals and other economic indicators. The UN set 2015 as the target for measurable progress. Given the data from 1972 - 2007, you need to predict a specific indicator for each of these goals in 2008 and 2012.\n",
    "\n",
    "This notebook contains three sections\n",
    "\n",
    " - [Digging into the Data](#digging)\n",
    " - [Making a simple model](#model)\n",
    " - [Starting to think about correlations](#correlations)\n",
    " \n",
    "We'll get started by loading the python modules in our analysis toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# data manipulation and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# graphix\n",
    "import matplotlib.pyplot as plt\n",
    "# import prettyplotlib as pplt\n",
    "import seaborn as sns\n",
    "import statsmodels.graphics.tsaplots as tsaplots\n",
    "\n",
    "# utility\n",
    "import os\n",
    "\n",
    "# notebook parameters\n",
    "pd.set_option('display.max_columns', 40) # number of columns in training set\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"digging\"></a>\n",
    "\n",
    "# Digging into the data\n",
    "\n",
    "---------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to go get that data. We've downloaded the data from [the competition data download page](http://www.drivendata.org/competitions/1/data/). We've put it in a folder called `data` in the same directory as this notebook. Now we can load in the data into the IPython notebook.  There are two files here:\n",
    "\n",
    " - **Training Set** - This contains all of the timeseries values for 241 countries, for 1337 indicators from 1970 - 2007. We've downloaded and unzipped the archive in our `data` folder, so we are working with the CSV file.\n",
    " \n",
    " - **Submission Rows** - This contains the rows and format for our submission. We need to predict the values in 2008 and 2012 for the time series with IDs in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"../data/TrainingSet.csv\", index_col=0)\n",
    "submission_labels = pd.read_csv(\"../out/SubmissionRows.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poking around, we can see that there are entries for 1972 - 2007 for many different countries for many different macroeconomic indicators. Also, **it's important to note**, there are lots of `NaN`'s. The best solutions will have good ways to handle missing values. For now, we'll ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission labels are simpler. First observation is we want to predict 2008 and 2012 (and not the years in between). Second observation is that the index values indentify specific rows in the training set. E.g., we want to predict the row in the training set that has the ID 559. Just to look that up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.loc[559]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can see that we want to predict the time series for the [Millennium Development Goal 7.8](http://unstats.un.org/unsd/mdg/Host.aspx?Content=Indicators/OfficialList.htm), which is about ensuring environmental sustainability.\n",
    "\n",
    "We're almost to the fun part. But first let's just write a little utility function to keep us sane. We noticed that the year columns have annoying titles (sorry!). We'll write a little function that will make it easier to grab any column that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_year_list(start, stop=None):\n",
    "    \"\"\" \n",
    "    make a list of column names for specific years\n",
    "    in the format they appear in the data frame start/stop inclusive\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(start, list):\n",
    "        data_range = start\n",
    "    elif stop:\n",
    "        data_range = range(start, stop+1)\n",
    "    else:\n",
    "        data_range = [start]\n",
    "    \n",
    "    yrs = []\n",
    "    \n",
    "    for yr in data_range:\n",
    "        yrs.append(\"{0} [YR{0}]\".format(yr))\n",
    "        \n",
    "    return yrs\n",
    "\n",
    "# ========== TEST CASES =======\n",
    "# one year\n",
    "print(generate_year_list(2008))\n",
    "\n",
    "# start and stop (inclusive)\n",
    "print(generate_year_list(1985, 1990))\n",
    "\n",
    "# custom year list\n",
    "print(generate_year_list([1985, 1990]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's start by looking at the time series that we want to predict. In `pandas` it's pretty easy to grab just those rows from the training set since we have the IDs in `submission_labels`. Also, to make our lives easier, we'll drop the info about what these rows are. We can always grab that again from the training data when we want. This way we're just looking at the numeric matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rows = training_data.loc[submission_labels.index]\n",
    "prediction_rows = prediction_rows[generate_year_list(1972, 2007)]\n",
    "prediction_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just get a visual sense of what some of these look like. We'll grab 10 of the series at random and plot them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a random sample of 10 of the timeseries\n",
    "np.random.seed(896)\n",
    "rand_rows = np.random.choice(prediction_rows.index.values, size=10)\n",
    "\n",
    "def plot_rows(data, ids=None, linestyle=\"-\", legend=True):\n",
    "    # get some colors for the lines\n",
    "    bmap = pplt.brewer2mpl.get_map('Set3','Qualitative', 10)\n",
    "    colors = bmap.mpl_colors\n",
    "    \n",
    "    if not None == ids:\n",
    "        get_rows = lambda: enumerate(ids)\n",
    "    else:\n",
    "        get_rows = lambda: enumerate(data.index.values)\n",
    "    \n",
    "    for i, r in get_rows():\n",
    "        # get the time series values\n",
    "        time_data = data.loc[r]\n",
    "\n",
    "        # create an x axis to plot along\n",
    "        just_years = [y[:4] for y in data.columns]\n",
    "        X = pd.DatetimeIndex(just_years)\n",
    "\n",
    "        # get time series info for labeling\n",
    "        country, descrip = training_data[[\"Country Name\", \"Series Name\"]].loc[r]\n",
    "\n",
    "        # plot the series\n",
    "        plt.plot(X, time_data, c=colors[i],\n",
    "                 label=\"{} - {}\".format(country, descrip), ls=linestyle)\n",
    "        plt.scatter(X, time_data, alpha=0.8, c=colors[i])\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(loc=0)\n",
    "    plt.title(\"Progress Towards Subset of MDGs\")\n",
    "\n",
    "plot_rows(prediction_rows, ids=rand_rows)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "\n",
    "# Making a simple model\n",
    "\n",
    "<hr>\n",
    "\n",
    "We'll start with a super simple model. If we only have data for 2007 and not before, we'll predict that value for 2008 and 2012. If we have more data, we'll do a simple linear regression using just the data points for 2006 and 2007. Our prediction will be that the line which passes through 2006 and 2007 also passes through 2008 and 2012. Hey, maybe you can think of something even more clever to **[move you up the leaderboard](http://www.drivendata.org/competitions/1/leaderboard/)**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(series):\n",
    "    point_2007 = series.iloc[-1]\n",
    "    point_2006 = series.iloc[-2]\n",
    "    \n",
    "    # if just one point, status quo\n",
    "    if np.isnan(point_2006):\n",
    "        predictions = np.array([point_2007, point_2007])\n",
    "    else:\n",
    "        slope = point_2007 - point_2006\n",
    "        \n",
    "        # one year\n",
    "        pred_2008 = point_2007 + slope\n",
    "        \n",
    "        # five years\n",
    "        pred_2012 = point_2007 + 5*slope\n",
    "        \n",
    "        predictions = np.array([pred_2008, pred_2012])\n",
    "\n",
    "    ix = pd.Index(generate_year_list([2008, 2012]))\n",
    "    return pd.Series(data=predictions, index=ix)\n",
    "        \n",
    "# let's try just these predictions on the first five rows\n",
    "test_data = prediction_rows.head()\n",
    "test_predictions = test_data.apply(simple_model, axis=1)\n",
    "\n",
    "# combine the data and the predictions\n",
    "test_predictions = test_data.join(test_predictions)\n",
    "\n",
    "# let's take a look at 2006, 2007, and our predictions\n",
    "test_predictions[generate_year_list([2006, 2007, 2008, 2012])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so our dead-simple regression is working. Let's see how it looks when we combine it with the data that we graphed earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the predictions\n",
    "predictions = prediction_rows.loc[rand_rows].apply(simple_model, axis=1)\n",
    "\n",
    "# plot the data\n",
    "plot_rows(prediction_rows, ids=rand_rows)\n",
    "\n",
    "# plot the predictions\n",
    "plot_rows(predictions, linestyle=\"--\", legend=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made some predictions!  These don't look too crazy, so let's write out a submission file that we can upload on on the [competition submissions page](http://www.drivendata.org/competitions/1/submissions/). We've put together a little utility function that will write out the submissions in the right format. It will accept the following data structures:\n",
    "\n",
    " - `pandas.DataFrame` - with index and columns that are already specified properly (as in this notebook)\n",
    " - `numpy.array` - with 2 columns (just the predictions) or 3 columns (id, 2008, 2012)\n",
    " - `list` - must be a list with two lists as its elements; the first, 2008, the second, 2012\n",
    " - `dict` - must have two keys that match the prediction indices ('2008 [YR2008]', '2012 [YR2012]') with values that are a list of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(preds, filename):\n",
    "    # load the submission labels\n",
    "    file_format = pd.read_csv(os.path.join(\"../out\", \"SubmissionRows.csv\"), index_col=0)\n",
    "    expected_row_count = file_format.shape[0]\n",
    "\n",
    "    if isinstance(preds, pd.DataFrame):\n",
    "        # check indices\n",
    "        assert(preds.index == file_format.index).all(), \\\n",
    "            \"DataFrame: Prediction indices must match submission format.\"\n",
    "        \n",
    "        # check columns\n",
    "        assert (preds.columns == file_format.columns).all(), \\\n",
    "            \"DataFrame: Column names must match submission format.\"\n",
    "        \n",
    "        final_predictions = preds\n",
    "        \n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        rows, cols = preds.shape\n",
    "        \n",
    "        if cols == 3:\n",
    "            assert (preds[:,0] == file_format.index.values).all(), \\\n",
    "                \"Numpy Array: First column must be indices.\"\n",
    "            \n",
    "            # now we know the indices are cool, ditch them\n",
    "            preds = preds[:,1:]\n",
    "        \n",
    "        assert rows == expected_row_count, \\\n",
    "            \"Numpy Array: The predictions must have the right number of rows.\"\n",
    "        \n",
    "        # put the predictions into the dataframe\n",
    "        final_predictions = file_format.copy()\n",
    "        final_predictions[generate_year_list([2008, 2012])] = preds\n",
    "            \n",
    "    elif isinstance(preds, list):\n",
    "        assert len(preds) == 2, \\\n",
    "            \"list: Predictions must be a list containing two lists\"\n",
    "        assert len(preds[0]) == expected_row_count, \\\n",
    "            \"list: There must be the right number of predictions in the first list.\"\n",
    "        assert len(preds[1]) == expected_row_count, \\\n",
    "            \"list: There must be the right number of predictions in the second list.\"\n",
    "    \n",
    "        # write the predictions\n",
    "        final_predictions = file_format.copy()\n",
    "        final_predictions[generate_year_list(2008)] = np.array(preds[0], dtype=np.float64).reshape(-1, 1)\n",
    "        final_predictions[generate_year_list(2012)] = np.array(preds[1], dtype=np.float64).reshape(-1, 1)\n",
    "        \n",
    "    elif isinstance(preds, dict):\n",
    "        assert preds.keys() == generate_year_list([2008, 2012]), \\\n",
    "            \"dict: keys must be properly formatted\"\n",
    "        assert len(preds[generate_year_list(2008)[0]]) == expected_row_count, \\\n",
    "            \"dict: length of value for 2008 must match the number of predictions\"\n",
    "        assert len(preds[generate_year_list(2012)[0]]) == expected_row_count, \\\n",
    "            \"dict: length of value for 2012 must match the number of predictions\"\n",
    "        \n",
    "        # create dataframe from dictionary\n",
    "        final_predictions = pd.DataFrame(preds, index=file_format.index)\n",
    "\n",
    "    final_predictions.to_csv(filename)\n",
    "    \n",
    "simple_predictions = prediction_rows.apply(simple_model, axis=1)\n",
    "write_submission_file(simple_predictions, \"Getting Started Benchmark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"correlations\"></a>\n",
    "\n",
    "# Starting to think about correlations\n",
    "\n",
    "<hr>\n",
    "\n",
    "We won't make any prediction based on this section of the notebook (that's up to you!), but one hypothesis you might have is that certain other timeseries are correlated with the ones we want to predict.\n",
    "\n",
    "As an example, say we are looking at GDP and percent of population that makes under a dollar a day (PLDD). You may think that an increase in GDP during 2006-2007 would entail a decrease PLDD during 2007-2008. Similarly a decrease during 2006-2007 in GDP would result in an increase in PLDD. \n",
    "\n",
    "We might call this a lagged correlation. As an experiment, we'll pick one country, Kenya, and make a correlation matrix between all of the indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenya_data = training_data[training_data[\"Country Name\"] == 'Kenya']\n",
    "kenya_values = kenya_data[generate_year_list(1972, 2007)].values\n",
    "\n",
    "# get the total number of time series we have for Kenya\n",
    "nseries = kenya_values.shape[0]\n",
    "\n",
    "# -1 as default\n",
    "lag_corr_mat = np.ones([nseries, nseries], dtype=np.float64)*-1\n",
    "\n",
    "# create a matrix to hold our lagged correlations\n",
    "for i in range(nseries):\n",
    "    for j in range(nseries):\n",
    "        # skip comparing a series with itself\n",
    "        if i!=j:\n",
    "            # get original (1972-2006) and shifted (1973-2007)\n",
    "            original = kenya_values[i,1:]\n",
    "            shifted = kenya_values[j,:-1]\n",
    "\n",
    "            # for just the indices where neither is nan\n",
    "            non_nan_mask = (~np.isnan(original) & ~np.isnan(shifted))\n",
    "\n",
    "            # if we have at least 2 data points\n",
    "            if non_nan_mask.sum() >= 2:\n",
    "                lag_corr_mat[i,j] = np.correlate(original[non_nan_mask], shifted[non_nan_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see what the most correlated lagged time series is for one of the rows we have to predict. I happen to know that the row with the index 131042 is one we need to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one of the indicators we are suppoed to predict\n",
    "to_predict_ix = 131042 \n",
    "\n",
    "# first, we get the index of that row in the correlation matrix\n",
    "i = np.where(kenya_data.index.values == to_predict_ix)[0][0]\n",
    "\n",
    "# then, we see which value in the matrix is the largest for that row\n",
    "j_max = np.argmax(lag_corr_mat[i,:])\n",
    "\n",
    "# finally, let's see what these correspond to\n",
    "max_corr_ix = kenya_data.index.values[j_max]\n",
    "\n",
    "# now write out what we've found\n",
    "fmt_string = \"In Kenya, the progress of '{}' is \"\\\n",
    "    \"most correlated with a change in '{}' during the year before.\"\n",
    "    \n",
    "print(fmt_string.format(kenya_data[\"Series Name\"][to_predict_ix], kenya_data[\"Series Name\"][max_corr_ix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, that's actually plausible! Awesome!\n",
    "\n",
    "That's all for now. It's your turn to uncover some of these interesting structures yourself and [gain some notoreity](http://www.drivendata.org/competitions/1/) along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking for a place to start, Hyndman's open textbook [\"Forecasting: principles and practice\"](https://www.otexts.org/fpp/) will definitely give you some ideas. Code snippets are in R, but translatable to Python.\n",
    "\n",
    "# Use this notebook as a jumping off point and [make some submissions](http://www.drivendata.org/competitions/1/). Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
