{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#recommend installing Anaconda for ease:\n",
    "#https://www.continuum.io/downloads\n",
    "#certain packages i.e. seaborn, do not come with the standard installation. A simple search for\n",
    "#how to 'install seaborn with anaconda' will easily find the installation instructions.\n",
    "#i.e. in this case:\n",
    "#http://seaborn.pydata.org/installing.html\n",
    "#conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Alexandru Agachi\n",
    "#m.a.agachi@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Why statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Why focus on data\n",
    "#\"Better to be in an expanding world and not quite in exactly the right field, than to be in a contracting world where\n",
    "#people's worst behavior comes out.\" Eric Weinstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Why healthcare\n",
    "#Ripe for data analysis\n",
    "#30% of world's data\n",
    "#\"The biomedical sciences have been the pillar of the health care system for a long time now. The new system will have \n",
    "#two equal pillars â€” the biomedical sciences and the data sciences.\" Dr Scott Zeger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Roadmap\n",
    "#Blueprint for statistical data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the packages we will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#use simplest tool available\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn import preprocessing\n",
    "#from sklearn.linear_model import LassoLarsCV\n",
    "#from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "#bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#All credit for the data, and our many thanks, go to the principal investigators who collected this data \n",
    "#and made it available\n",
    "\n",
    "#1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "#2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "#3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "#4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\n",
    "# cd Downloads/Heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Data management is an integral part of your research process\n",
    "#Choices you make here will influence your entire study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cleveland = pd.read_csv('processed.cleveland.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cleveland.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hungary = pd.read_csv('processed.hungarian.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hungary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "switzerland = pd.read_csv('processed.switzerland.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "switzerland.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "va = pd.read_csv('processed.va.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "va.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#va.replace('?', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([cleveland, hungary, va, switzerland])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we rename all columns to make them more legible\n",
    "df.columns = ['age', 'sex', 'chest_pain', 'rest_bp', 'cholesterol', 'fasting_bs',\n",
    "              'rest_ecg', 'max_heart_rate', 'exercise_angina', 'st_depression',\n",
    "              'slope', 'fluoroscopy', 'defect', 'diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#What sort of study was this?\n",
    "\n",
    "#1. Observational or experimental\n",
    "\n",
    "#Observational and experimental data\n",
    "\n",
    "#to know if your data is obersevational or experimental, you ask if the explanatory variable was manipulated or not\n",
    "\n",
    "#In a true experiment, 3 conditions:\n",
    "#1. only one variable is manipulated\n",
    "#2. we have a control group\n",
    "#3. random* assignment (analysis stage in randomized trials: check clss imbalances, and if any found\n",
    "#consider incl. in model as explanatory variables, for statistical control)\n",
    "#In theory, in this case one can determine causality\n",
    "\n",
    "#Quasi experiment:\n",
    "#1. only one variable is manipulated\n",
    "#2. control group\n",
    "#3. no random assignment; groups pre selected. i.e. drug users study.\n",
    "#To improve a quasi experimental design: add confounding variables; have a control group; use a pre-test/post-test design\n",
    "#confounder=control variable=covariate=third variable=lurking variable\n",
    "\n",
    "#in an observational study the regression line only describes data you see. it cannot be used to predict result of \n",
    "#intervention\n",
    "\n",
    "\n",
    "#*randomization works best as your sample size approaches infinity. for small sizes, imbalances in the groups\n",
    "#can occur. if you check randomized studies, one of first steps is to check for imbalances between groups\n",
    "#on covariates. this is also why we can conclude that variables are associated, but hardly that one causes the other\n",
    "#statistical control: include unbalanced covariates as additional explanatory variables in the study.\n",
    "\n",
    "#2. Longitudinal or cross sectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Then data analysis? Wrong.\n",
    "#Background research. Always starts with background research. Domain knowledge.\n",
    "#Detrano et al.: http://www.ajconline.org/article/0002-9149(89)90524-9/pdf\n",
    "#Sundaram and Kakade: http://www2.rmcil.edu/dataanalytics/v2015/papers/Clinical_Decision_Support_For_Heart_Disease.pdf\n",
    "#Soni et al.: http://www.enggjournals.com/ijcse/doc/IJCSE11-03-06-120.pdf\n",
    "#Value the time of domain experts.\n",
    "#Dataset we'll focus on today. look at txt file AND at explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#First stage in a study is always exploratory data analysis\n",
    "#We typically receive the data without context, in a raw file that we cannot easily interpret\n",
    "#The five steps of exploratory data analysis\n",
    "#1. ?\n",
    "#2. organizing and summarizing the data\n",
    "#3. Looking for important features and patterns\n",
    "#4. Looking for exceptions\n",
    "#5. Interpreting these findings in the context of the research question at hand\n",
    "#This can be summarized in a notebook or even an initial data report for everyone involved in the data project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# should be 5 times more than no columns\n",
    "print(len(df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#number of observations to variables largely exceeds heuristic rule of 5 to 1\n",
    "#see Victoria Stodden (2006)\n",
    "#https://web.stanford.edu/~vcs/thesis.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#convert all variables to numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#mark all variables as numeric data, and signify, for the relevant ones, that they are categorical rather than \n",
    "#quantitative variables\n",
    "#errors='coerce' tells pandas to return invalid values as NaN rather than as the input values themselves\n",
    "#crucial to do this step otherwise subsequent analyses will not work properly. i.e. pandas would interpret missing \n",
    "#values as strings\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df['sex'] = pd.to_numeric(df['sex'], errors='coerce').astype('category')\n",
    "df['chest_pain'] = pd.to_numeric(df['chest_pain'], errors='coerce').astype('category')\n",
    "df['rest_bp'] = pd.to_numeric(df['rest_bp'], errors='coerce')\n",
    "df['cholesterol'] = pd.to_numeric(df['cholesterol'], errors='coerce')\n",
    "df['fasting_bs'] = pd.to_numeric(df['fasting_bs'], errors='coerce').astype('category')\n",
    "df['rest_ecg'] = pd.to_numeric(df['rest_ecg'], errors='coerce').astype('category')\n",
    "df['max_heart_rate'] = pd.to_numeric(df['max_heart_rate'], errors='coerce')\n",
    "df['exercise_angina'] = pd.to_numeric(df['exercise_angina'], errors='coerce').astype('category')\n",
    "df['st_depression'] = pd.to_numeric(df['st_depression'], errors='coerce')\n",
    "df['slope'] = pd.to_numeric(df['slope'], errors='coerce').astype('category')\n",
    "df['fluoroscopy'] = pd.to_numeric(df['fluoroscopy'], errors='coerce').astype('category')\n",
    "df['defect'] = pd.to_numeric(df['defect'], errors='coerce').astype('category')\n",
    "df['diagnosis'] = pd.to_numeric(df['diagnosis'], errors='coerce').astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "let's check for missing/outlier values\n",
    "\n",
    "Why bother?\n",
    "Nearly all statistical methods assume complete information. In the presence of missing data:\n",
    "1. parameter estimates may be biased\n",
    "2. statistical power weakens\n",
    "3. precision of confidence intervals is diminished\n",
    "\n",
    "Three types of missing variables:\n",
    "1. Missing completely at random (MCAR): p(missing data on Y) independent of p(Y value) or p(other variables values)\n",
    "but: p(missing data on Y) may be linked to p(missing data on other variables in dataset))\n",
    "2. Missing at random (MAR): p(missing data on Y) independent of value of Y after controlling for other variables\n",
    "3. Not missing at random (NMAR)\n",
    "\n",
    "If MAR, missing data is ignorable and there is no need to model missing data mechanism\n",
    "if NMAR, missing data mechanism is not ignorable and one must develop v good understanding of missing data process to model it\n",
    "\n",
    "Standard options:\n",
    "1. listwise deletion. works well if MCAR, which is rarely true, and can delete significant part of our sample\n",
    "2. imputation.\n",
    "2. a. marginal mean imputation: leads to biased estimates of variance and covariance and should be avoided\n",
    "2. b. conditional mean imputation: we regress missing values on values of all other variables present in dataset.\n",
    "    MCAR assumption. Generalized least squares usually shows good results.\n",
    "Overall issue with imputation methods: underestimate standard errors and overstimate test statistics.\n",
    "\n",
    "Advanced options:\n",
    "1. Multiple imputation\n",
    "2. Maximum likelihood\n",
    "3. Bayesian simulation\n",
    "4. Hot deck (selects at random, with replacement, a value from observations with similar values for other variables)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "\n",
    "http://www.bu.edu/sph/files/2014/05/Marina-tech-report.pdf\n",
    "\n",
    "http://www.statsmodels.org/dev/imputation.html\n",
    "\n",
    "\"The only really good solution to the missing data problem is not to have any. So in the design and execution of \n",
    "research projects, it is essential to put great effort into minimizing the occurrence of missing data. \n",
    "Statistical adjustments can never make up for sloppy research.\" Paul Allison, 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check for missing vars:\n",
    "df['age'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#manual quartile split\n",
    "age_binned = pd.qcut(df['age'], 4, labels = [\"1=0%tile\", \"2=25%tile\", \"3=50%tile\", \n",
    "                                                        \"4=75%tile\"]).value_counts(sort=False)\n",
    "print(age_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['sex'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['sex'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['chest_pain'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['chest_pain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['rest_bp'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['rest_bp'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['cholesterol'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['cholesterol'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['fasting_bs'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['fasting_bs'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#standard value_counts() function drops missing values. To avoid this you can add dropna=False argument to function.\n",
    "df['fasting_bs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['rest_ecg'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['rest_ecg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['max_heart_rate'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['max_heart_rate'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['exercise_angina'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['exercise_angina'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['exercise_angina'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['st_depression'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['st_depression'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['slope'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['slope'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['slope'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['fluoroscopy'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['fluoroscopy'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['fluoroscopy'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['defect'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['defect'].isnull().value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['defect'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['diagnosis'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the variables slope, defect and fluoroscopy have 33-47% of missing values\n",
    "#context. Why?\n",
    "#can we correct for this without introducing other biases into our dataset?\n",
    "#Here we will decide to eliminate these variables.\n",
    "#Data analysts are mere mortals too. Cannot fix everything.\n",
    "df_red = df[['age', 'sex', 'chest_pain', 'rest_bp', 'cholesterol', 'fasting_bs', 'rest_ecg', 'max_heart_rate', \n",
    "            'exercise_angina', 'st_depression', 'diagnosis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_red.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#rest_bp, cholesterol, fasting_bs, rest_ecg, max_heart_rate, exercise_angina, st_depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#rest_ecg is a categorical variable with only 2% of missing values.\n",
    "#we make the choice to impute missing values with a straightforward method: the mode\n",
    "#we are conscious this may introduce biases but due to low number of missing values, in what is\n",
    "#a small range categorical variable (no extreme outliers possible) we feel comfortable doing this\n",
    "df_red['rest_ecg'].fillna(df_red['rest_ecg'].mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_red['rest_ecg'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#let's see if there is any overlap between missing variables relative to observations, and what would happen if we \n",
    "#were to limit our dataset to observations with non missing values\n",
    "df_clean = df_red[df_red['rest_bp'].notnull() & df_red['cholesterol'].notnull() & df_red['fasting_bs'].notnull() & df['max_heart_rate'].notnull() & df['exercise_angina'].notnull() &\n",
    "                    df['st_depression'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "this is not approach we would use in a real world study, but for exploratory purposes in this tutorial\n",
    "we can retain 85% of observations that were properly recorded regarding all variables\n",
    "I will add here an example with conditional mean imputation for one of the variables\n",
    "Statsmodels very limited in options = very manual work.\n",
    "From version 0.8.0 onwards MICE function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "now that we cleaned the data we can move on to univariate analysis -> one variable at a time\n",
    "\n",
    "data reporting tells you what is happening, but data analysis tells you why it is happening\n",
    "\n",
    "\n",
    "Descriptive statistics\n",
    "\n",
    "a parameter is calculated from the population, while a statistic from a sample\n",
    "\n",
    "In studying samples, we assume the Central Limit Theorem holds: if you draw enough samples, from\n",
    "a population, and each sample is large enough, the distribution of the statistics of the samples\n",
    "will be normally distributed.\n",
    "normal curve discovered around 1720 by Abraham de Moivre. Around 1870, Adolph Quetelet thought\n",
    "of using it as the curve of an ideal histogram\n",
    "\n",
    "center-spread-modality\n",
    "3 measures of center: mean, median and mode\n",
    "symmetry or skewness. skewed right (i.e. salaries), skewed left (i.e. age of natural deaths)\n",
    "peakness or modality. unimodal, bimodal, uniform...\n",
    "\n",
    "now we will aim to get an idea of the shape, center, and spread of these variables\n",
    "we will analyze the shape visually by checking for modality and skewness\n",
    "we will check for measures of center such as mean, median and mode\n",
    "we will check the spread through the standard deviation for quantitative variables\n",
    "SD: how far away observations are from their average\n",
    "in a normal distribution roughly 68% are within 1 SD and 95% within 2 SDs.\n",
    "\n",
    "Quantitative variables\n",
    "shape, center and spread \n",
    "histogram\n",
    "\n",
    "Categorical variables\n",
    "mode\n",
    "bar chart or frequency distribution\n",
    "\n",
    "Bias affects all measurements the same way, while chance errors vary from measurement to measurement\n",
    "therefore bias cannot be noticed by looking just at measurements, we need an external/theoretical\n",
    "benchmark as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "rules for visualizing data:\n",
    "\n",
    "for visualizing one variable: \n",
    "if it is categorical we use a bar chart i.e. sns's countplot function\n",
    "if it is quantitative, we can combine a kernel density estimate and a histogram with sns's distplot function\n",
    "\n",
    "for visualizing two variables:\n",
    "C-Q: bivariate bar graph with sns factorplot (bin/collapse explanatory variable), categories\n",
    "on x axis, and mean of response variable on y axis\n",
    "Q-Q: scatterplot with sns regplot\n",
    "C-C: you can plot them one a time. problem with a bivariate graph is that mean has no meaning in context of a \n",
    "categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Gender\n",
    "#given that gender is a categorical variable, we use a countplot to visualize it\n",
    "#we always plot the variable of interest on the x axis, and the count or frequency on the y axis\n",
    "sns.countplot(x='sex', data = df_clean)\n",
    "plt.suptitle('Frequency of observations by gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Diagnosis\n",
    "#we check the counts per each value of the variable. sort=False tells pandas not to sort the results by values. \n",
    "#normalize = True tells it to return the relative frequencies rather than the absolute counts\n",
    "#if we had not cleaned the data, we could add parameter dropna=False so that value_counts does not drop null values\n",
    "df_clean['diagnosis'].value_counts(sort=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='diagnosis', data=df_clean)\n",
    "plt.suptitle('Frequency distribution of diagnosis state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Let's look at age now\n",
    "#the describe request gives us the count, mean, std, min, max, as well as the quartiles for the\n",
    "#respective value distribution\n",
    "df_clean['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sns distplot function combines matplotlib hist() function with sns kdeplot() function.\n",
    "sns.distplot(df_clean['age'])\n",
    "plt.suptitle('Distribution of age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean['max_heart_rate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_clean['max_heart_rate'])\n",
    "plt.suptitle('Distribution of maximal heart rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# shows how many obeservations are in each category\n",
    "sns.swarmplot('diagnosis', data=df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#rules for visualizing two variables (recap):\n",
    "# C-Q: bivariate bar graph with sns factorplot (bin/collapse explanatory variable), categories on x axis, and mean \n",
    "#of response variable on y axis\n",
    "# Q-Q: scatterplot with sns regplot\n",
    "#C-C: you can plot them one a time. problem with a bivariate graph is that mean has no meaning\n",
    "#in context of a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='sex', y='age', kind='bar', data=df_clean)\n",
    "plt.suptitle('Gender vs age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#categorical explanatory variable 'sex' and quantitative response variable 'rest_bp'\n",
    "#kind = bar asks for a bar graph and ci=None suppresses error bars\n",
    "sns.factorplot(x='sex', y='rest_bp', data=df_clean, kind='bar', ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean['chest_pain'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We use a regplot to plot two quantitative variables, age and allcrimes, while also having a regression line suggesting\n",
    "#any association present\n",
    "#we always plot the explanatory variable on the x axis, and the response variable on the y axis\n",
    "sns.regplot(x='age', y='cholesterol', data=df_clean[df_clean['cholesterol']!=0]) # also remove unreasonable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#how can we gain a better idea of how two categorical variables interact?\n",
    "df_clean.groupby('sex')['diagnosis'].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='cholesterol', y = 'sex', data=df_clean)\n",
    "plt.suptitle('Cholesterol levels by gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#describing the dataset/the variables of the dataset, after we group the values in the dataset\n",
    "#by age category\n",
    "df_clean.groupby('diagnosis').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#before starting to manipulate the dataset itself, we make a copy, and will work on the copy\n",
    "#rather than the original reduced dataset\n",
    "df_clean_copy = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Now we can move to inferential statistics\n",
    "\n",
    "#Hypothesis testing:\n",
    "#1. define null hypothesis and alternate hypothesis\n",
    "#2. choose sample\n",
    "#3. analyze evidence\n",
    "#4. interpret results\n",
    "\n",
    "#Typical H0: there is no relationship between the explanatory and response variables\n",
    "#Typical H1: there is a statistically significant relationship\n",
    "\n",
    "#Bivariate statistical tools:\n",
    "#ANOVA; chi-square; correlation coefficient\n",
    "\n",
    "#a result is statistically significant if it is unlikely to be the result of chance\n",
    "\n",
    "#p value is also the type 1 error rate: the number of times we would be wrong in rejecting the null hypothesis \n",
    "#when it is true\n",
    "#p=0.03: if we reject the null hypothesis, we would be correct 97/100 times.\n",
    "#Type 1 vs Type 2 errors\n",
    "\n",
    "#F = variation among sample means/variation within sample groups\n",
    "#ANOVA F Test: are the differences among the sample means due to true differences among the\n",
    "#population means, or merely due to sampling variability\n",
    "#p value of ANOVA: probability of getting an F value as large or larger if H0 is true\n",
    "#probability of finding this value if there is no difference between sample means\n",
    "\n",
    "#before performing these analyses, one needs to use the .dropna() function to include only\n",
    "#valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we will test the null hypothesis that age and diagnosis are not related.\n",
    "#the type of variables we have (explanatory/response and categorical/quantitative for each) determines the type of \n",
    "#statistical tools we will use\n",
    "#Explanatory categorical and response quantitative: ANOVA\n",
    "#Explanatory categorical and response categorical: Chi Square test\n",
    "#Explanatory quantitative and response categorical: classify/bin explanatory variable and use chi square test\n",
    "#Explanatory quantitative and response quantitative: pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Going the wrong way? (age is response var, but doesnt matter because we only check correlation)\n",
    "# C stands for categorical\n",
    "test1 = smf.ols(formula='age ~ C(diagnosis)', data=df_clean_copy).fit()\n",
    "print(test1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "F-statistic: variation / variation in sample\n",
    "\n",
    "Prob(F-statistic): p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Whenever the explanatory variable has more than 2 levels, we need to also perform post hoc statistical tests to \n",
    "#better understand the relationship between the explanatory variable and the response variable\n",
    "#we know the groups tested are different overall, but not exactly where/how they are different\n",
    "#for explanatory variables with multiples levels, F test and p value do not tell us why the\n",
    "#group means are not equal, or how. there are many ways in which this can be the case.\n",
    "#How are they associated per level of the explanatory variable?\n",
    "#post hoc tests aim to protect against type 1 error when explanatory variable is multilevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now we examine the means and standard deviations\n",
    "grouped1_mean = df_clean_copy.groupby('diagnosis').mean()['age']\n",
    "print(grouped1_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped1_std = df_clean_copy.groupby('diagnosis').std()['age']\n",
    "print(grouped1_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#given that we have an explanatory categorical variable with multiple levels, we use the \n",
    "#tuckey hsd test\n",
    "#other tests:\n",
    "#Holm T\n",
    "#Least Significant Difference\n",
    "\n",
    "tuckey1 = multi.MultiComparison(df_clean_copy['age'], df_clean_copy['diagnosis'])\n",
    "res1 = tuckey1.tukeyhsd()\n",
    "print(res1.summary())\n",
    "# we cannot reject the null hypothesis for the last 3 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We will now test another hypothesis:\n",
    "#Hypothesis(0)(a): the presence of chest pain and the diagnosis (0 or 1) are independent\n",
    "#Alternative Hypothesis 1: presence of chest pain and diagnosis are not independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "#Paradox: you can get better results with great feature engineering and a poor model than with poor feature engineering\n",
    "#but a great model\n",
    "#\"A feature is an attribute that is useful to your problem\" Dr. Jason Brownlee\n",
    "#\"The algorithms we used are very standard for Kagglers...We spent most of our efforts in feature engineering.\"\n",
    "#Xavier Conort, #1 Kaggler in 2013\n",
    "#Aims to convert data attributes into data features\n",
    "#Aims to optimize data modelling\n",
    "#Requires understanding of the dataset and research problem, and understanding of model you plan on using\n",
    "#can be domain driven, or data driven\n",
    "#i.e. for SVM with linear kernel you need to manually construct nonlinear interactions between\n",
    "#features and feed them as input to your SVM model. An SVM with polynomial kernel will naturally\n",
    "#capture them.\n",
    "#other example: SVMs are very sensitive to dimensions of features, while DT/RFs are not\n",
    "#With tabular data you combine, aggregate, split and/or decompose features in order to create new ones\n",
    "#Given an output y and a feature x, you can try the following transforms first:\n",
    "#e^x, log(x), x^2, x^3\n",
    "#an indicator of the usefulness of the transformation is if the correlation between y and x'\n",
    "#is higher than the correlation between y and x\n",
    "#best way to validate this is to check your model error with or without the transformed feature\n",
    "#http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "#http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/\n",
    "\n",
    "diagnosis_dic = {0:0, 1:1, 2:1, 3:1, 4:1}\n",
    "df_clean_copy['diagnosis_binary'] = df_clean_copy['diagnosis'].map(diagnosis_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['diagnosis_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#contingency table of observed counts\n",
    "#the crosstab function allows us to cross one variable with another\n",
    "#when creating contingency tables, we put the response variable first (therefore vertical in table), \n",
    "#and the explanatory variable second, therefore horizontal at the top of the table.\n",
    "ct1 = pd.crosstab(df_clean_copy['diagnosis_binary'], df_clean_copy['chest_pain'])\n",
    "print(ct1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum = ct1.sum(axis=0)\n",
    "colpct = ct1/colsum\n",
    "print(colpct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "#Expected counts: p assuming events are independent. p(1) * p(2) | column total*row total/table total\n",
    "#Chi square statistic summarizes this. difference between our obersavtion and what we would expect if H0 is true\n",
    "#We rely on the p value, as different distributions define whether the chi square itself is large or not\n",
    "\n",
    "print('chi-square value, p value, expected counts')\n",
    "cs1 = scipy.stats.chi2_contingency(ct1)\n",
    "print(cs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Explanatory variable with multiple levels!\n",
    "#we would have to do a pairwise comparison between every two groups of the explanatory\n",
    "#variable, vs the response variable\n",
    "#This would be a Bonferroni adjustment - we adjust p value we use by number of pairwise comparisons, and test these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct2 = pd.crosstab(df_clean_copy['diagnosis_binary'], df_clean_copy['sex'])\n",
    "print(ct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum2 = ct2.sum(axis=0)\n",
    "colpct2 = ct2/colsum2\n",
    "print(colpct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "print('chi-square value, p value, expected counts')\n",
    "cs2 = scipy.stats.chi2_contingency(ct2)\n",
    "print(cs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Moderators\n",
    "# (moderators, hidden var, lurking var)\n",
    "#a moderator is a third variable that affects the direction and/or strength between your explanatory and response variables\n",
    "#the question is, is our response variable associated with our explanatory variable, for each level of our third variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#let's see if sex is a moderator in the statistically significant relationship between chest pain and diagnosis\n",
    "df_clean_copy_men = df_clean_copy[df_clean_copy['sex'] == 0]\n",
    "len(df_clean_copy_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy_women = df_clean_copy[df_clean_copy['sex'] == 1]\n",
    "len(df_clean_copy_women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#contingency table of observed counts\n",
    "#when creating contingency tables, we put the response variable first (therefore vertical in table), \n",
    "#and the explanatory variable second, therefore horizontal at the top of the table.\n",
    "ct3 = pd.crosstab(df_clean_copy_men['diagnosis_binary'], df_clean_copy_men['chest_pain'])\n",
    "print(ct3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum = ct3.sum(axis=0)\n",
    "colpct = ct3/colsum\n",
    "print(colpct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "print('chi-square value, p value, expected counts')\n",
    "cs3 = scipy.stats.chi2_contingency(ct3)\n",
    "print(cs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#contingency table of observed counts\n",
    "#when creating contingency tables, we put the response variable first (therefore vertical in table), \n",
    "#and the explanatory variable second, therefore horizontal at the top of the table.\n",
    "ct4 = pd.crosstab(df_clean_copy_women['diagnosis_binary'], df_clean_copy_women['chest_pain'])\n",
    "print(ct4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum = ct4.sum(axis=0)\n",
    "colpct = ct4/colsum\n",
    "print(colpct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "print('chi-square value, p value, expected counts')\n",
    "cs4 = scipy.stats.chi2_contingency(ct4)\n",
    "print(cs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy_women.groupby('chest_pain')['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the relationship between chest pain and the diagnosis holds for both levels of the sex variables, hence it is not a moderator.\n",
    "#we would test for moderator variables in the case of a quantitative response variable the same way\n",
    "#divide the population into the sublevels of the third variables\n",
    "#conduct an smf.ols test for each to see if the relationship is statistically significant for each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#identifying a confounding variable does not allow to establish causation, just to get closer to a causal connection.\n",
    "#due to infinite number of possible lurking variables, observational studies cannot rly establish causation\n",
    "#a lurking of confounding variable is a third variable that is associated with both the explanatory and response \n",
    "#variables.\n",
    "#i.e. x=firefighters; y=damage caused by a fire. plot would suggest more firefighters causes more fire damage.\n",
    "#in reality there is a third confounding variable that influences both, seriousness of the fire.\n",
    "#In a study we want to demonstrate that our statistical relationship is valid even after controlling for confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now we will test whether there is a relationship between two quantitative variables, age and cholesterol\n",
    "#for this we use the pearson correlation test\n",
    "#r, going from -1 to 1 only tells us whether the two variables are linearly related. they may be related in nonlinear ways\n",
    "#therefore it's always important to look at r in parallel with a scatterplot of the two variables\n",
    "#r squared is a measure of how much variability in one variable can be explained by the other variable\n",
    "#to calculate the pearson coefficient we need to remove all missing values\n",
    "#Please remember that when two variables are correlated it is possible that:\n",
    "#X causes Y or Y causes X\n",
    "#Z causes both X and Y\n",
    "#X and Y are correlated by chance - a spurious correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_r = df_clean_copy[df_clean_copy['cholesterol']!=0]\n",
    "\n",
    "scat1 = sns.regplot(x='age', y = 'cholesterol', fit_reg=True, data = df_r)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Cholesterol')\n",
    "plt.suptitle('Relationship between age and cholesterol')\n",
    "scat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the r coefficient is a measure of association, of how closely points are clustered around a line\n",
    "#correlations are always between -1 and 1\n",
    "#r measures solely linear association\n",
    "#association, not causation\n",
    "#it is a number without units\n",
    "#it can mislead in presence of outliers or non linear association -> always draw a scatter plot\n",
    "#when you look at a scatter plot you look at direction form and strength of relationship\n",
    "#ecological correlations based on averages can be misleading and overstate strength of associations\n",
    "#for individual units\n",
    "print('association between age and cholesterol')\n",
    "print(scipy.stats.pearsonr(df_r['age'], df_r['cholesterol']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Linear regression:\n",
    "#multivariate linear regression for quantitative response variable\n",
    "#logistic regression for binary categorical response variable\n",
    "\n",
    "#Assumptions:\n",
    "\n",
    "#Normality: residuals from our linear regression model are normally distributed. if they are not,\n",
    "#our model may be misspecified.\n",
    "\n",
    "#Linearity: association between explanatory and response variable is linear\n",
    "\n",
    "#Homoscedasticity (or assumption of constant variance): variability in the response variable is the same at all levels\n",
    "#of the explanatory variable. i.e. if residuals are spread around the regression line in a \n",
    "#similar manner as you move along the x axis (values of the explanatory variable)\n",
    "\n",
    "#Independence: observations are not correlated with each other. Longitudinal data can violate this assumption, as well \n",
    "#as hierarchical nesting/clustering data i.e. looking at students by classes. this assumption is the most serious\n",
    "#to be violated, and also cannot be fixed by modifying the variables. the data structure itself is the problem.\n",
    "#We have to contend with:\n",
    "\n",
    "#Multicollinearity: explanatory variables are highly correlated with each other. this can mess up your parameter estimates\n",
    "#or make them highly unstable. Signs: 1. highly associated variable not significant. 2. negative regression coefficient\n",
    "#that should be positive. 3. taking out an explanatory variable drastically changes the results.\n",
    "\n",
    "#Outliers: can affect your regression line, meaning it will \n",
    "#multiple regression model allows us to find the relationship between one explanatory variable and the \n",
    "#reponse variable, while controlling (holding constant at 0) all the other variables.\n",
    "#categorical sex (1 and 2); age restricted to 18-25 group: each variable needs to include a meaningful\n",
    "#value of 0, so as to make it easier to interpret the coefficients\n",
    "#for a categorical variable, we can just recode one of the values to be 0\n",
    "#for a quantitative variable, we have to center it. Centering = subtracting the mean of a variable\n",
    "#from the value of the variable. We are therefore recoding it so that its mean=0.\n",
    "#we only center explanatory variables not response one\n",
    "#in logistic regression we always need to code response binary variable so that 0 means no outcome and 1 outcome occurred\n",
    "#this is true whether outcome is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We will create a multiple regression model, investigating the relationship between our explanatory variables and \n",
    "#our response variable diagnosis\n",
    "#we will first center the explanatory variables. for categorical variables, one of the categories needs to be 0, for \n",
    "#quantitative variables, we need to subtract the mean from each value.\n",
    "\n",
    "#Notes: \n",
    "#do not center the response variable.\n",
    "#is using logistic regression, do recode the binary response variable to make sure one class is coded as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#categorical variables: sex, chest_pain, fasting_bs, rest_ecg, exercise_angina\n",
    "#quantitative variables: age, rest_bp, cholesterol, max_heart_rate, st_depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['chest_pain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we need one categorical var as 0 for ANOVA\n",
    "recode_chest_pain = {1:0, 2:1, 3:2, 4:3}\n",
    "df_clean_copy['chest_pain_p'] = df_clean_copy['chest_pain'].map(recode_chest_pain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['fasting_bs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['rest_ecg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['exercise_angina'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "center the distributions of continuous vars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['age_c'] = df_clean_copy['age'] - df_clean_copy['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['rest_bp_c'] = df_clean_copy['rest_bp'] - df_clean_copy['rest_bp'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['cholesterol_c'] = df_clean_copy['cholesterol'] - df_clean_copy['cholesterol'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['max_heart_rate_c'] = df_clean_copy['max_heart_rate'] - df_clean_copy['max_heart_rate'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy['st_depression_c'] = df_clean_copy['st_depression'] - df_clean_copy['st_depression'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy_c = df_clean_copy[['age_c', 'sex', 'chest_pain_p', 'rest_bp_c', 'cholesterol_c',\n",
    "                                       'fasting_bs', 'rest_ecg', 'max_heart_rate_c', 'exercise_angina',\n",
    "                                       'st_depression_c', 'diagnosis_binary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model1 = smf.ols(formula = 'age_c ~ sex', data = df_clean_copy_c).fit()\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add variables one at a time:\n",
    "model2 = smf.ols(formula = 'age_c ~ sex + cholesterol_c', data=df_clean_copy_c).fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model3 = smf.ols(formula = 'age_c ~ sex + C(chest_pain_p) + (rest_bp_c) + cholesterol_c + fasting_bs + C(rest_ecg) + \\\n",
    "                            max_heart_rate_c + exercise_angina + st_depression_c + diagnosis_binary', data = df_clean_copy_c).fit()\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We identified some explanatory variables that are associated with age, but our model overall barely explains 27%\n",
    "#of the variation in the response variable\n",
    "#let's run some diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Q-Q plot for normality\n",
    "fig1 = sm.qqplot(model3.resid, line='r')\n",
    "#red line represents residuals we would expect if model residuals were normally distributed\n",
    "#our residuals below deviate significantly from red line, especially at lower and higher quantiles, meaning they do not\n",
    "#follow a normal distribution. This means curvilinear association we saw is not fully explained by our model. We could add\n",
    "#more explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#simple plot of residuals\n",
    "stdres = pd.DataFrame(model3.resid_pearson)\n",
    "plt.plot(stdres, 'o', ls='None')\n",
    "l = plt.axhline(y=0, color='r')\n",
    "plt.ylabel('Standardized Residual')\n",
    "plt.xlabel('Observation Number')\n",
    "#resid_pearson normalizes our model's residuals\n",
    "#ls='none' means points will not be connected\n",
    "#we expect most residuals to fall within 2sd of the mean. More than 2 are outliers, and more than 3 extreme outliers.\n",
    "#if more than 1% of our observations have standardized residuals with an absolute value greater than 2.5, or more than 5%\n",
    "#have one greater than or equal to 2, there is evidence that the fit of the model is poor. largest cause of this is ommission\n",
    "#of important explanatory variables in our model.\n",
    "#standardized residuals in linear regression will always be linear, and the line will be horizontal\n",
    "#normalizing or standardizing residuals amounts to making them have a mean of 0 and sd of 1 so as to fit a normal standard distribution\n",
    "#if residuals show a strong pattern (up, down, polynomial) then it is a good indication of nonlinearity\n",
    "#in the underlying relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#leverage plot\n",
    "fig4 = sm.graphics.influence_plot(model3, size=8)\n",
    "print(fig4)\n",
    "#we see that we have extreme outliers, but they are low leverage, meaning they do not have an undue influence on our\n",
    "#estimation of the regression model.\n",
    "#we also have high leverage observations, but they are not outliers.\n",
    "#we have no observations that are both high leverage and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now let's focus on our actual response variable in the study\n",
    "#since it is a binary variable, we will need to use a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lreg1 = smf.logit(formula = 'diagnosis_binary ~ C(chest_pain_p)', data = df_clean_copy_c).fit()\n",
    "print(lreg1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lreg2 = smf.logit(formula = 'diagnosis_binary ~ age_c + sex + C(chest_pain_p) + cholesterol_c', data = df_clean_copy_c).fit()\n",
    "print(lreg2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lreg3 = smf.logit(formula = 'diagnosis_binary ~ age_c + sex + C(chest_pain_p) + rest_bp_c + \\\n",
    "                  cholesterol_c + fasting_bs + C(rest_ecg) + max_heart_rate_c + \\\n",
    "                  exercise_angina + st_depression_c', data = df_clean_copy_c).fit()\n",
    "print(lreg3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#however, for logistic regression it makes much more sense to calculate the odds ratio\n",
    "#if OR=1, the model is not statistically significant\n",
    "#if OR<1, the response variable becomes less likely as the explanatory one increases\n",
    "#if OR>1, the response variable becomes more likely as the explanatory one increases\n",
    "print('Odds Ratios')\n",
    "print(np.exp(lreg3.params))\n",
    "#Interpretation of OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# odd ratios with 95% confidence intervals\n",
    "params = lreg3.params\n",
    "conf = lreg3.conf_int()\n",
    "conf['OR'] = params\n",
    "conf.columns = ['Lower CI', 'Upper CI', 'OR']\n",
    "print (np.exp(conf))\n",
    "#we have 95% confidence that the sex odds ratio will be between 2.24 and 6.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#simple plot of residuals\n",
    "stdres = pd.DataFrame(lreg3.resid_pearson)\n",
    "plt.plot(stdres, 'o', ls='None')\n",
    "l = plt.axhline(y=0, color='r')\n",
    "plt.ylabel('Standardized Residual')\n",
    "plt.xlabel('Observation Number')\n",
    "#resid_pearson normalizes our model's residuals\n",
    "#ls='none' means points will not be connected\n",
    "#we expect most residuals to fall within 2sd of the mean. More than 2 are outliers, and more than 3 extreme outliers.\n",
    "#if more than 1% of our observations have standardized residuals with an absolute value greater than 2.5, or more than 5%\n",
    "#have one greater than or equal to 2, there is evidence that the fit of the model is poor. largest cause of this is ommission\n",
    "#of important explanatory variables in our model.\n",
    "#standardized residuals in linear regression will always be linear, and the line will be horizontal\n",
    "#if residuals show a strong pattern (up, down, polynomial) then it is a good indication of nonlinearity\n",
    "#in the underlying relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_mice = df_red[df_red['rest_bp'].notnull() & df_red['cholesterol'].notnull() & df_red['fasting_bs'].notnull() & \\\n",
    "                  df['max_heart_rate'].notnull() & df['exercise_angina'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_mice.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_mice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#to be able to use MICE you will need to update the statsmodels coming with Anaconda, to 0.8.0\n",
    "import statsmodels.imputation.mice as mice\n",
    "import statsmodels\n",
    "from statsmodels.base.model import LikelihoodModelResults\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from collections import defaultdict\n",
    "formula_mice = 'diagnosis_binary ~ age_c + sex + C(chest_pain_p) + rest_bp_c + \\\n",
    "                  cholesterol_c + fasting_bs + C(rest_ecg) + max_heart_rate_c + \\\n",
    "                  exercise_angina + st_depression_c'\n",
    "mice = statsmodels.mice.MICE(formula_mice, slf.logit, df_clean_mice)\n",
    "results = mice.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Machine learning encompasses a wide range of statistical methods. These can be used for:\n",
    "#1. Describe associations\n",
    "#2. Search for patterns\n",
    "#3. Make predictions\n",
    "#We typically do not use ML with hypotheses in mind. instead we learn from the data\n",
    "#We learn from the test set.\n",
    "#Accuracy = test error rate. The rate at which it correctly classifies or estimates.\n",
    "#Goal is to minimize test error rate.\n",
    "#Linear regression: accuracy = mean squared error\n",
    "#Variance = change in parameter estimates across different data sets\n",
    "#Bias = how far off model estimated values are from true values\n",
    "#ideally we want low variance and low bias, but they are negatively associated. As one decreases, the other increases.\n",
    "#Generally, complexity of model leads to high variance and low bias\n",
    "#Simple models will have lower variance, but also be more biased.\n",
    "#Logistic regression: accuracy = how well the model classifies observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Supervised Prediction includes:\n",
    "#Linear regression\n",
    "#Pattern recognition\n",
    "#Discriminant analysis\n",
    "#Multivariate function estimation\n",
    "#Supervised ML techniques\n",
    "#Decision trees\n",
    "#Like linear regression, decision trees are designed for supervised prediction problems.\n",
    "#Root node, and terminal nodes or leaves.\n",
    "#Growing the tree process: binary splits maximize correct classification; all cut points are tested; subgroups showing\n",
    "#similar outcomes are generated\n",
    "#Validating the tree: cross validation guards against overfit. A random subset is tested and only 'branches' that improve\n",
    "#the classification are retained\n",
    "#Selected sub tree is the lowest probability of misclassification\n",
    "#Trees allow the handling of many variables that cannot be done as efficiently in linear regression. They can also uncover\n",
    "#constellations of variables that can predict high or low rates of the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Strengths of decision trees\n",
    "#Can select from a large number of variables those and their interactions that are most important in determining the\n",
    "#target or response variable to be explained\n",
    "#They are easy to interpret and visualize, especially when the tree is small\n",
    "#Can handle large data sets well and can predict both binary, categorical target variables and also quantitative ones\n",
    "#Limitations: small changes in the data can lead to different splits and this can undermine the interpretability of the model\n",
    "#and decision trees are not very reproducible on future data\n",
    "#Note: decision trees cannot handle missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_clean_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_sex = pd.get_dummies(df_clean_copy['sex'], prefix = 'sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_chest_pain = pd.get_dummies(df_clean_copy['chest_pain'], prefix = 'chest_pain')\n",
    "df_fasting_bs = pd.get_dummies(df_clean_copy['fasting_bs'], prefix = 'fasting_bs')\n",
    "df_rest_ecg = pd.get_dummies(df_clean_copy['rest_ecg'], prefix = 'rest_ecg')\n",
    "df_exercise_angina = pd.get_dummies(df_clean_copy['exercise_angina'], prefix='exercise_angina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_clean_copy, df_sex, df_chest_pain, df_fasting_bs, df_rest_ecg, df_exercise_angina], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_dt = df_merged[['age', 'sex_0.0', 'sex_1.0', 'chest_pain_1.0', 'chest_pain_2.0', 'chest_pain_3.0', \\\n",
    "                       'chest_pain_4.0', 'rest_bp', 'cholesterol', 'fasting_bs_0.0', 'fasting_bs_1.0', \\\n",
    "                      'rest_ecg_1.0', 'rest_ecg_2.0', 'max_heart_rate', 'exercise_angina_0.0', 'exercise_angina_1.0',\\\n",
    "                       'st_depression', 'diagnosis_binary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors = df_dt.ix[:, df_dt.columns != 'diagnosis_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = df_dt['diagnosis_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(pred_train.shape, pred_test.shape, tar_train.shape, tar_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "classifier = classifier.fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Random forests\n",
    "#Forests of trees\n",
    "#Splits on only ONE variable in each node. Variable with largest association with Target among\n",
    "#candidate variables. Only among variables randomly selected to be tested for that node.\n",
    "#First a subset of explanatory variables is selected at random\n",
    "#Next the node is split with the Best variable of the subset. After this node is split, a new list of subset variables\n",
    "#is selected at random to split on the next node.\n",
    "#typical k fold values: 5 or 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(n_estimators = 25)\n",
    "classifier2 = classifier.fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions2 = classifier2.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(pred_train, tar_train)\n",
    "#display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trees = range(25)\n",
    "accuracy = np.zeros(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(trees)):\n",
    "    classifier = RandomForestClassifier(n_estimators = idx + 1)\n",
    "    classifier = classifier.fit(pred_train, tar_train)\n",
    "    predictions = classifier.predict(pred_test)\n",
    "    accuracy[idx] = sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.plot(trees, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lasso regression\n",
    "#penalized regression method\n",
    "#supervised learning method\n",
    "#shrinkage and selection method\n",
    "#shrinkage = constraints on parameters that shrinks coefficients to 0\n",
    "#selection = identifies most imp. variables associated with response variable\n",
    "#Can increase prediction accuracy and improve model interpretability vs standard OLS\n",
    "#When lambda=0, it becomes OLS regression\n",
    "#Bias increases and variance decreases as lambda increases\n",
    "#in Lasso regression, penalty is not fair if variables are not on the same scale\n",
    "#standardize all predictor variables to have means equal to 0 and sd = 1\n",
    "#Lasso regression has several algorithms, among them LAR (least angle regression-)\n",
    "#sklearn library refers to the penalty term as 'alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Limitations of lasso regression\n",
    "#1. Selection of variables is 100% statistically driven\n",
    "#2. If predictors are correlated, lasso arbitrarily selects one\n",
    "#3. Estimating p values is not straightforward\n",
    "#4. Different selection methods or statistical softwares can provide different results\n",
    "#5. No guarantee that selected model is not overfitted nor that it's the best model\n",
    "#All regression models can produce meaningless models without human intervention\n",
    "#Best approach is a combination of ML, human intervention, and independent application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors['age'] = preprocessing.scale(predictors['age'].astype('float64'))\n",
    "predictors['sex'] = preprocessing.scale(predictors['sex'].astype('float64'))\n",
    "predictors['chest_pain'] = preprocessing.scale(predictors['rest_bp'].astype('float64'))\n",
    "predictors['cholesterol'] = preprocessing.scale(predictors['cholesterol'].astype('float64'))\n",
    "predictors['fasting_bs'] = preprocessing.scale(predictors['fasting_bs'].astype('float64'))\n",
    "predictors['rest_ecg'] = preprocessing.scale(predictors['rest_ecg'].astype('float64'))\n",
    "predictors['max_heart_rate'] = preprocessing.scale(predictors['max_heart_rate'].astype('float64'))\n",
    "predictors['exercise_angina'] = preprocessing.scale(predictors['exercise_angina'].astype('float64'))\n",
    "predictors['st_depression'] = preprocessing.scale(predictors['st_depression'].astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, \n",
    "                                                             test_size=.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#specify lasso regression model\n",
    "model = LassoLarsCV(cv=10, precompute=False).fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dict(zip(predictors.columns, model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plot coefficient regression\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "ax = plt.gca()\n",
    "plt.plot(m_log_alphas, model.coef_path_.T)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k', label = 'alpha CV')\n",
    "plt.ylabel('Regression Coefficients')\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.title('Regression Coefficients Progression for Lasso Paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plot mean squared error for each fold\n",
    "m_log_alphascv = -np.log10(model.cv_alphas_)\n",
    "plt.figure()\n",
    "plt.plot(m_log_alphascv, model.cv_mse_path_, ':')\n",
    "plt.plot(m_log_alphascv, model.cv_mse_path_.mean(axis=-1), 'k', label = 'Average across the folds',\n",
    "         linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha CV')\n",
    "plt.legend()\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Mean squared error on each fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# MSE from training and test data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
    "test_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
    "print ('training data MSE')\n",
    "print(train_error)\n",
    "print ('test data MSE')\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# R-square from training and test data\n",
    "rsquared_train=model.score(pred_train,tar_train)\n",
    "rsquared_test=model.score(pred_test,tar_test)\n",
    "print ('training data R-square')\n",
    "print(rsquared_train)\n",
    "print ('test data R-square')\n",
    "print(rsquared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "groupby()....size()\n",
    "groupby().size()*100/len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
