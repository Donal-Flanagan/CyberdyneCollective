{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#I recommend installing Anaconda for ease:\n",
    "#https://www.continuum.io/downloads\n",
    "#certain packages i.e. seaborn, do not come with the standard installation. A simple search for\n",
    "#how to 'install seaborn with anaconda' will easily find the installation instructions.\n",
    "#i.e. in this case:\n",
    "#http://seaborn.pydata.org/installing.html\n",
    "#conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#With regards to the data, and the study during which this data was collected, rights belong to:\n",
    "#Friedmann, Peter. Criminal Justice Drug Abuse Treatment Studies (CJ-DATS): Step 'N Out, 2002-2006 [United States]. \n",
    "#ICPSR30221-v1. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2011-07-27. \n",
    "#They made the entire dataset openly available at:\n",
    "#http://doi.org/10.3886/ICPSR30221.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#indicate the directory where your data folder, in this case the file 'stepnout.csv', is located\n",
    "#in this case, it sits in a folder on the dropbox cloud.\n",
    "# cd Dropbox/Data visualization/Course 5/_32ec92f8b8c6d83a374ae0989bec1447_StepNOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the packages we will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load the full dataset\n",
    "full_dataset = pd.read_csv('stepnout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#show the first ten rows of the dataset to get an idea of the variables and their values\n",
    "full_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#rename all columns to small caps\n",
    "full_dataset.rename(columns = lambda x: x.lower(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#same result, different way of doing it\n",
    "#full_dataset.columns = map(str.lower, full_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#same result, different way of doing it\n",
    "#df = df.rename(columns=lambda x: x.replace('$', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#try to get all columns, one sees there are 691\n",
    "full_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print all columns/variables\n",
    "print(full_dataset.describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#initial exploratory data analysis\n",
    "#frequency distributions for categorical variables\n",
    "#graphical representations\n",
    "#calculations of center and spread for quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we select 17 variables of interest\n",
    "reduced_dataset = full_dataset[['cond', 'csex', 'age', 'clive', 'majsup',  'allarrests', 'anyarrest', 'alldrugs', 'anydrugs', 'allcrimes', 'anycrime',\n",
    "                               'arrest_9mo', 'reincarc_9mo', 'num_arrest', 'num_reincarc', 'violent_charge',\n",
    "                               'property_charge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we take a look at the first 30 rows\n",
    "reduced_dataset.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we check the length of the dataset\n",
    "print(len(reduced_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we confirm the number of columns\n",
    "print(len(reduced_dataset.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we rename some of the columns to give them more meaningful names\n",
    "reduced_dataset.rename(columns = {'csex': 'sex', 'clive': 'living_situation', 'majsup': 'support'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#convert all variables to numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#mark all variables as numeric data, and signify, for the relevant ones, that they are categorical\n",
    "#rather than quantitative variables\n",
    "#errors='coerce' tells pandas to return invalid values as NaN rather than as the input values themselves\n",
    "reduced_dataset['cond'] = pd.to_numeric(reduced_dataset['cond'], errors='coerce').astype('category')\n",
    "reduced_dataset['sex'] = pd.to_numeric(reduced_dataset['sex'], errors='coerce').astype('category')\n",
    "reduced_dataset['age'] = pd.to_numeric(reduced_dataset['age'], errors='coerce')\n",
    "reduced_dataset['living_situation'] = pd.to_numeric(reduced_dataset['living_situation'], errors='coerce').astype('category')\n",
    "reduced_dataset['support'] = pd.to_numeric(reduced_dataset['support'], errors='coerce').astype('category')\n",
    "reduced_dataset['allarrests'] = pd.to_numeric(reduced_dataset['allarrests'], errors='coerce')\n",
    "reduced_dataset['anyarrest'] = pd.to_numeric(reduced_dataset['anyarrest'], errors='coerce').astype('category')\n",
    "reduced_dataset['alldrugs'] = pd.to_numeric(reduced_dataset['alldrugs'], errors='coerce')\n",
    "reduced_dataset['anydrugs'] = pd.to_numeric(reduced_dataset['anydrugs'], errors='coerce').astype('category')\n",
    "reduced_dataset['allcrimes'] = pd.to_numeric(reduced_dataset['allcrimes'], errors='coerce')\n",
    "reduced_dataset['anycrime'] = pd.to_numeric(reduced_dataset['anycrime'], errors='coerce').astype('category')\n",
    "reduced_dataset['arrest_9mo'] = pd.to_numeric(reduced_dataset['arrest_9mo'], errors='coerce').astype('category')\n",
    "reduced_dataset['reincarc_9mo'] = pd.to_numeric(reduced_dataset['reincarc_9mo'], errors='coerce').astype('category')\n",
    "reduced_dataset['num_arrest'] = pd.to_numeric(reduced_dataset['num_arrest'], errors='coerce')\n",
    "reduced_dataset['num_reincarc'] = pd.to_numeric(reduced_dataset['num_reincarc'], errors='coerce')\n",
    "reduced_dataset['violent_charge'] = pd.to_numeric(reduced_dataset['violent_charge'], errors='coerce').astype('category')\n",
    "reduced_dataset['property_charge'] = pd.to_numeric(reduced_dataset['property_charge'], errors='coerce').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now let's look at the counts and frequency distributions for these variables\n",
    "#for this, we will aim to get an idea of the shape, center, and spread of these variables\n",
    "#we will analyze the shape visually by checking for modality and skewness\n",
    "#we will check for measure of center such as mean, median and mode\n",
    "#we will check the spread through the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reduced_dataset['cond'].value_counts(sort=False, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reduced_dataset['cond'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#rules for visualizing data:\n",
    "\n",
    "#for visualizing a variable: \n",
    "#if it is categorical we use a bar chart i.e. sns's countplot function\n",
    "#if it is quantitative, we can combine a kernel density estimate and a histogram with sns's \n",
    "#distplot function\n",
    "\n",
    "#for visualizing two variables:\n",
    "# C-C: bivariate bar graph with sns factorplot\n",
    "# C-Q: bivariate bar graph with sns factorplot\n",
    "# Q-Q: scatterplot with sns regplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#given that the study condition is a categorical variable, we use a count plot to visualize it.\n",
    "sns.countplot(x='cond', data=reduced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we check the counts per each value of the variable. sort=False tells pandas not to sort the results\n",
    "#by values. normalize = True tells it to return the relative frequencies rather than the absolute counts\n",
    "reduced_dataset['sex'].value_counts(sort=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print all age values as a list to look through them\n",
    "lis = [x for x in reduced_dataset['age']]\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the describe request gives us the count, mean, std, min, max, as well as the quartiles for the\n",
    "#respective value distribution\n",
    "reduced_dataset['age'].dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#given that age is a quantitative variable, we use a distplot to visualize it.\n",
    "sns.distplot(reduced_dataset['age'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We use a regplot to plot two quantitative variables, age and allcrimes, while also having a regression line suggesting\n",
    "#any association present\n",
    "sns.regplot(x='age', y='allcrimes', data=reduced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#categorical explanatory variable 'sex' and quantitative response variable 'allcrimes'\n",
    "sns.factorplot(x='sex', y='allcrimes', data=reduced_dataset, kind='bar', ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='sex', y='allcrimes', kde='bar', data=reduced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#before starting to manipulate the dataset itself, we make a copy, and will work on the copy\n",
    "#rather than the original reduced dataset\n",
    "rdc = reduced_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct1 = rdc.groupby('anyarrest').size()*100/len(rdc['anyarrest'])\n",
    "print(ct1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['anyarrest'].value_counts(sort=False, dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['cond'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['anyarrest'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['anyarrest'] = rdc['anyarrest'].replace(11, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['anyarrest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "living_dic = {1: 1, 2:2, 3:1, 4:1, 5:1, 6:1, 7:1, 8:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['living_situation'] = rdc['living_situation'].map(living_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.swarmplot('living_situation', data=reduced_dataset)\n",
    "plt.xlabel('Living Situation')\n",
    "plt.title('Distribution of the study sample by type of living arrangement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='living_situation', y='allcrimes', data=rdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.countplot('living_situation', data=rdc)\n",
    "plt.xlabel('Living Situation')\n",
    "plt.title('Distribution of the study sample by type of living arrangement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['age'].describe()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def age_group(x):\n",
    "    if x < 30:\n",
    "        return 20\n",
    "    elif x < 40:\n",
    "        return 30\n",
    "    elif x < 50:\n",
    "        return 40\n",
    "    elif x < 61:\n",
    "        return 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['age'] = rdc['age'].map(age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#describing the dataset/the variables of the dataset, after we group the values in the dataset\n",
    "#by age category\n",
    "rdc.groupby('age').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#quartile split\n",
    "rdc['age_unprocessed'] = reduced_dataset['age']\n",
    "print('Age - 4 categories - quartiles')\n",
    "age_binned = rdc['age_binned'] = pd.qcut(rdc['age_unprocessed'], 4, labels = [\"1=0%tile\", \"2=25%tile\",\n",
    "                                                 \"3=50%tile\", \"4=75%tile\"]).value_counts(sort=False, dropna=True)\n",
    "print(age_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Hypothesis testing:\n",
    "#1. specify the null hypothesis and the alternate hypothesis\n",
    "#2. choose a sample\n",
    "#3. assess the evidence\n",
    "#4. draw conclusions\n",
    "#Definition: assessing evidence provided by the data in favor or against each hypothesis\n",
    "#about the population\n",
    "\n",
    "#a result is statistically significant if it is unlikely to have occurred by chance\n",
    "#p value is also the type 1 error rate: the number of times we would be wrong in rejecting the\n",
    "#null hypothesis when it is true\n",
    "#p=0.03: if we reject the null hypothesis, we would be correct 97/100 times.\n",
    "\n",
    "#Bivariate statistical tools:\n",
    "#ANOVA; chi-square; correlation coefficient\n",
    "\n",
    "#ANOVA F Test: are the differences among the sample means due to true differences among the\n",
    "#population means, or merely due to sampling variability\n",
    "#F is the variation among samples means divided by the variation within groups\n",
    "#for explanatory variables with multiples levels, F test and p value do not tell us why the\n",
    "#group means are not equal, or how. there are many ways in which this can be the case.\n",
    "\n",
    "#before performing these analyses, one needs to use the .dropna() function to include only\n",
    "#valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we will test the null hypothesis that the study condition (subject or control) and number of crimes are not related.\n",
    "test1_data = rdc[['cond', 'allcrimes']].dropna()\n",
    "len(test1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test1 = smf.ols(formula='allcrimes ~ C(cond)', data=test1_data).fit()\n",
    "print(test1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now we examine the means and stds\n",
    "grouped1_mean = test1_data.groupby('cond').mean()\n",
    "print(grouped1_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped1_std = test1_data.groupby('cond').std()\n",
    "print(grouped1_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we repeat the same analysis with arrests\n",
    "test2_data = rdc[['cond', 'allarrests']].dropna()\n",
    "test2 = smf.ols(formula = 'allarrests ~ C(cond)', data=test2_data).fit()\n",
    "print(test2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped2_mean = test2_data.groupby('cond').mean()\n",
    "print(grouped2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped2_std = test2_data.groupby('cond').std()\n",
    "print(grouped2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we now try to use the grouped age variable as well\n",
    "rdc['age'] = rdc['age'].astype('category')\n",
    "test3_data = rdc[['age', 'allcrimes']].dropna()\n",
    "test3 = smf.ols(formula = 'allcrimes ~ C(age)', data=rdc).fit()\n",
    "print(test3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#given that we have an explanatory categorical variable with multiple levels, we use the \n",
    "#tuckey hsd test\n",
    "tuckey1 = multi.MultiComparison(test3_data['allcrimes'], test3_data['age'])\n",
    "res1 = tuckey1.tukeyhsd()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We will now test two other hypotheses:\n",
    "#Hypothesis(0)(a): the study condition (0 or 1) and the committing of a crime are independent \n",
    "#i.e. there is no relationship between them\n",
    "#Hypothesis(0)(b): there is no relationship between age and being arrested during the study \n",
    "#period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#contingency table of observed counts\n",
    "#when creating contingency tables, we put the response variable first (therefore vertical in \n",
    "#table), and the explanatory variable second, therefore horizontal at the top of the table.\n",
    "ct1 = pd.crosstab(rdc['anycrime'], rdc['cond'])\n",
    "print(ct1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum = ct1.sum(axis=0)\n",
    "colpct = ct1/colsum\n",
    "print(colpct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "print('chi-square value, p value, expected counts')\n",
    "cs1 = scipy.stats.chi2_contingency(ct1)\n",
    "print(cs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now the second hypothesis test\n",
    "rdc['age'] = rdc['age'].astype('category')\n",
    "rdc['anyarrest'] = rdc['anyarrest'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#contingency table of observed counts\n",
    "ct2 = pd.crosstab(rdc['anyarrest'], rdc['age'])\n",
    "print(ct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum2 = ct2.sum(axis=0)\n",
    "colpct2 = ct2/colsum2\n",
    "print(colpct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "print('chi-square value, p value, expected counts')\n",
    "cs2 = scipy.stats.chi2_contingency(ct2)\n",
    "print(cs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We therefore cannot reject the null hypothesis, but given that the explanatory variable has several levels, \n",
    "#we cannot know why the null hypothesis was not rejected\n",
    "#we therefore perform a 2 by 2 comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub1 = rdc.copy()\n",
    "recode1 = {20:20, 30:30}\n",
    "sub1['comp1v'] = sub1['age'].map(recode1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct3 = pd.crosstab(sub1['cond'], sub1['comp1v'])\n",
    "print(ct3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#column percentages\n",
    "colsum3 = ct3.sum(axis=0)\n",
    "colpct3 = ct3 / colsum3\n",
    "print(colpct3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#chi square test\n",
    "print('chi square value, p value, expected values')\n",
    "cs3 = scipy.stats.chi2_contingency(ct3)\n",
    "print(cs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recode2 = {20:20, 40:40}\n",
    "sub1['comp2v'] = sub1['age'].map(recode2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct4 = pd.crosstab(sub1['anyarrest'], sub1['comp2v'])\n",
    "print(ct4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colsum4 = ct4.sum(axis=0)\n",
    "colpct4 = ct4/colsum4\n",
    "print(colpct4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('chi square value, p value, expected values')\n",
    "cs4 = scipy.stats.chi2_contingency(ct4)\n",
    "print(cs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recode3 = {20:20, 50:50}\n",
    "sub1['compv3'] = sub1['age'].map(recode3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct5 = pd.crosstab(sub1['anyarrest'], sub1['compv3'])\n",
    "print(ct5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colsum5 = ct5.sum(axis=0)\n",
    "colpct5 = ct5/colsum5\n",
    "print(colpct5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('chi square value, p value, expected values')\n",
    "cs5 = scipy.stats.chi2_contingency(ct5)\n",
    "print(cs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recode4 = {30:30, 40:40}\n",
    "sub1['compv4'] = sub1['age'].map(recode4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct6 = pd.crosstab(sub1['anyarrest'], sub1['compv4'])\n",
    "print(ct6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colsum6 = ct6.sum(axis=0)\n",
    "colpct6 = ct6 / colsum6\n",
    "print(colpct6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('chi square value, p value, expected values')\n",
    "cs6 = scipy.stats.chi2_contingency(ct6)\n",
    "print(cs6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recode6 = {30:30, 50:50}\n",
    "sub1['compv6'] = sub1['age'].map(recode6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct7 = pd.crosstab(sub1['anyarrest'], sub1['compv6'])\n",
    "print(ct7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colsum7 = ct7.sum(axis=0)\n",
    "colpct7 = ct7/colsum7\n",
    "print(colpct7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('chi square value, p value, expected values')\n",
    "cs7 = scipy.stats.chi2_contingency(ct7)\n",
    "print(cs7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recode7 = {40:40, 50:50}\n",
    "sub1['compv7'] = sub1['age'].map(recode7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ct8 = pd.crosstab(sub1['anyarrest'], sub1['compv7'])\n",
    "print(ct8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colsum8 = ct8.sum(axis=0)\n",
    "colpct8 = ct8 / colsum8\n",
    "print(colpct8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('chi square value, p value, expected values')\n",
    "cs8 = scipy.stats.chi2_contingency(ct8)\n",
    "print(cs8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now we will test whether there is a relationship between two quantitative variables, age_unprocessed and allcrimes\n",
    "#for this we use the pearson correlation test\n",
    "#r, going from -1 to 1 only tells us whether the two variables are linearly related. they may be related in nonlinear ways\n",
    "#therefore it's always important to look at r in parallel with a scatterplot of the two variables\n",
    "#r squared is a measure of how much variability in one variable can be explained by the other variable\n",
    "#to calculate the pearson coefficient we need to remove all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scat1 = sns.regplot(x='age_unprocessed', y='allcrimes', fit_reg=True, data=rdc)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.title('Relationship between age and number of crimes')\n",
    "scat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_pearson_test = rdc[['age_unprocessed', 'allcrimes']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('association between age and number of crimes')\n",
    "print(scipy.stats.pearsonr(data_pearson_test['age_unprocessed'],\n",
    "                                            data_pearson_test['allcrimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#a moderator is a third variable that affects the direction and/or strength between your explanatory and response variables\n",
    "#the question is, is our response variable associated with our explanatory variable, for each level of our third variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#let's see if gender is a moderator variable for test group -> allcrimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub11 = rdc[['cond', 'sex', 'allcrimes']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub12 = sub11[sub11['sex']==1]\n",
    "sub13 = sub11[sub11['sex']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_12 = smf.ols(formula='allcrimes ~ C(cond)', data=sub12).fit()\n",
    "print(model_12.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_13 = smf.ols(formula='allcrimes ~ C(cond)', data=sub13).fit()\n",
    "print(model_13.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub12.groupby('cond').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub12.groupby('cond').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub13.groupby('cond').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub13.groupby('cond').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='cond', y='allcrimes', kind='bar', data=sub12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='cond', y='allcrimes', kind='bar', data=sub13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#we would test for moderator variables with the chi square test the same way\n",
    "#divide the population into the sublevels of the third variables\n",
    "#conduct a chi square test for each to see if the relationship is statistically significant for each level\n",
    "#we would visualize it with a linegraph factorplot(kind='point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#to know if your data is obersevational or experimental, you ask if the explanatory variable was manipulated or not\n",
    "#data reporting tells you what is happening, but data analysis tells you why it is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#randomization works best as your sample size approaches infinity. for small sizes, imbalances in the groups\n",
    "#can occur. if you check randomized studies, one of first steps is to check for imbalances between groups\n",
    "#on covariates. this is also why we can conclude that variables are associated, but hardly that one causes the other\n",
    "#statistical control: include unbalanced covariates as additional explanatory variables in the study.\n",
    "#In a true experiment, 3 conditions:\n",
    "#1. only one variable is manipulated\n",
    "#2. we have a control group\n",
    "#3. random assignment\n",
    "#In theory, in this case one can determine causality\n",
    "#Quasi experiment:\n",
    "#1. only one variable is manipulated\n",
    "#2. control group\n",
    "#3. no random assignment; groups pre selected. i.e. drug users study.\n",
    "#To improve a quasi experimental design: add confounding variables; have a control group; use a pre-test/post-test design\n",
    "#confounder=control variable=covariate=third variable=lurking variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#identifying a confounding variable does not allow to establish causation, just to get closer to a causal connection.\n",
    "#due to infinite number of possible lurking variables, observational studies cannot rly establish causation\n",
    "#a lurking of confounding variable is a third variable that is associated with both the explanatory and response \n",
    "#variables.\n",
    "#i.e. x=firefighters; y=damage caused by a fire. plot would suggest more firefighters cuases more fire damage.\n",
    "#in reality there is a third lurking variable that influences both, seriousness of the fire.\n",
    "#In a study we want to demonstrate that our statistical relationship is valid even after controlling for confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Linear regression:\n",
    "#multivariate linear regression for quantitative response variable\n",
    "#logistic regression for binary categorical response variable\n",
    "#Assumptions:\n",
    "#Normality: residuals from our linear regression model are normally distributed. if they are not,\n",
    "#our model may be misspecified.\n",
    "#Linearity: association between explanatory and response variable is linear\n",
    "#Homoscedasticity (or assumption of constant variance): variability in the response variable is the same at all levels\n",
    "#of the explanatory variable. i.e. if spread of residuals values increases as you move along x axis, assumption is \n",
    "#false.\n",
    "#Independence: observations are not correlated with each other. Longitudinal data can violate this assumption, as well \n",
    "#as hierarchical nesting/clustering data i.e. looking at students by classes. this assumption is the most serious\n",
    "#to be violated, and also cannot be fixed by modifying the variables. the data structure itself is the problem.\n",
    "#We have to contend with:\n",
    "#Multicollinearity: explanatory variables are highly correlated with each other. this can mess up your parameter estimates\n",
    "#or make them highly unstable. Signs: 1. highly associated variable not significant. 2. negative regression coefficient\n",
    "#that should be positive. 3. taking out an explanatory variable drastically changes the results.\n",
    "#Outliers: can affect your regression line, meaning it will not fit the data as well as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#multiple regression model allows us to find the relationship between one explanatory variable and the \n",
    "#reponse variable, while controlling (holding constant at 0) all the other variables.\n",
    "#categorical sex (1 and 2); age restricted to 18-25 group: each variable needs to include a meaningful\n",
    "#value of 0, so as to make it easier to interpret the coefficients\n",
    "#for a categorical variable, we can just recode one of the values to be 0\n",
    "#for a quantitative variable, we have to center it. Centering = subtracting the mean of a variable\n",
    "#from the value of the variable. We are therefore recoding it so that its mean=0.\n",
    "#Note: do not center the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We will create a multiple regression model, investigating the relationship between the study group 'cond', 'age', 'sex', \n",
    "#and the quantitative response variable 'allcrimes'. We will then do the same for 'allarrests'.\n",
    "#we will first center the explanatory variables. for categorical variables, one of the categories needs to be 0, for \n",
    "#quantitative variables, we need to subtract the mean from each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc_linear = rdc[['cond', 'age_unprocessed', 'sex', 'allcrimes', 'allarrests']].dropna()\n",
    "len(rdc_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc_linear['age_unprocessed_c'] = rdc['age_unprocessed']-rdc['age_unprocessed'].mean()\n",
    "print(rdc_linear['age_unprocessed_c'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc_linear['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recode20 = {1:1, 2:0}\n",
    "rdc_linear['sex_c'] = rdc_linear['sex'].map(recode20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model20 = smf.ols(formula = 'allcrimes ~ C(cond)', data=rdc_linear).fit()\n",
    "print(model20.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model21 = smf.ols(formula = 'allcrimes ~ C(cond)+age_unprocessed_c', data=rdc_linear).fit()\n",
    "print(model21.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model22 = smf.ols(formula = 'allcrimes ~ C(cond)+age_unprocessed_c+C(sex)', data=rdc_linear).fit()\n",
    "print(model22.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model23 = smf.ols(formula = 'allcrimes ~ C(cond)+age_unprocessed_c + I(age_unprocessed_c**2)+C(sex)', data=rdc_linear).fit()\n",
    "print(model23.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model24 = smf.ols(formula = 'allarrests ~ C(cond)', data=rdc_linear).fit()\n",
    "print(model24.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model25 = smf.ols(formula = 'allarrests ~ C(cond)+age_unprocessed_c', data=rdc_linear).fit()\n",
    "print(model25.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model26 = smf.ols(formula = 'allarrests ~ C(cond)+age_unprocessed_c+C(sex)', data=rdc_linear).fit()\n",
    "print(model26.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model27 = smf.ols(formula = 'allarrests ~ C(cond)+age_unprocessed_c + I(age_unprocessed_c**2)+C(sex)', data=rdc_linear).fit()\n",
    "print(model27.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#group means and sd\n",
    "print('Mean')\n",
    "ds1 = rdc_linear.groupby('cond').mean()\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Standard Deviation')\n",
    "ds2 = rdc_linear.groupby('cond').std()\n",
    "print(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#For each response variable, allcrimes and allarrests, we choose the model that gave us the highest overall\n",
    "#explanatory power, and run further tests to check for evidence of model misspecification.\n",
    "#If model is correctly specified, residuals are not correlated with explanatory variables.\n",
    "#If data fails to meet regression assumptions, or misses explanatory variables, we have model misspecification.\n",
    "#Intercept = value of response variable when all explanatory variables are held constant at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Q-Q plot for normality\n",
    "fig4 = sm.qqplot(model23.resid, line='r')\n",
    "#red line represents residuals we would expect if model residuals were normally distributed\n",
    "#our residuals below deviate significantly from red line, especially at lower and higher quantiles, meaning they do not\n",
    "#follow a normal distribution. This means curvilinear association we saw is not fully explained by our model. We could add\n",
    "#more explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#normalizing or standardizing values makes them fit a standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#simple plot of residuals\n",
    "stdres = pd.DataFrame(model23.resid_pearson)\n",
    "plt.plot(stdres, 'o', ls='None')\n",
    "l = plt.axhline(y=0, color='r')\n",
    "plt.ylabel('Standardized Residual')\n",
    "plt.xlabel('Observation Number')\n",
    "#resid_pearson normalizes our model's residuals\n",
    "#ls='none' means points will not be connected\n",
    "#we expect most residuals to fall within 2sd of the mean. More than 2 are outliers, and more than 3 extreme outliers.\n",
    "#if more than 1% of our observations have standardized residuals with an absolute value greater than 2.5, or more than 5%\n",
    "#have one greater than or equal to 2, there is evidence that the fit of the model is poor. largest cause of this is ommission\n",
    "#of important explanatory variables in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#additional regression diagnostic plots\n",
    "#fig1 = plt.figure(figsize(12,8))\n",
    "fig1 = sm.graphics.plot_regress_exog(model23, 'age_unprocessed_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#leverage plot\n",
    "fig3 = sm.graphics.influence_plot(model23, size=8)\n",
    "print(fig3)\n",
    "#we see that we have extreme outliers, but they are low leverage, meaning they do not have an undue influence on our\n",
    "#estimation of the regression model.\n",
    "#we also have high leverage observations, but they are not outliers.\n",
    "#we have no observations that are both high leverage and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now we will apply logistic regression models to the binary response variables anycrime and anyarrest\n",
    "#the data management has already been performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc_logistic['anycrime'] = pd.to_numeric(rdc_logistic['anycrime'], errors='coerce')\n",
    "rdc_logistic['anyarrest'] = pd.to_numeric(rdc_logistic['anyarrest'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdc_logistic['cond'] = rdc_logistic['cond'].astype('category')\n",
    "lreg1 = smf.logit(formula='anyarrest ~ C(cond)', data=rdc_logistic).fit()\n",
    "print(lreg1.summary())\n",
    "#equation would be anyarrest = -1.0259-0.1268*cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#however, for logistic regression it makes much more sense to calculate the odds ratio\n",
    "#if OR=1, the model is not statistically significant\n",
    "#if OR<1, the response variable becomes less likely as the explanatory one increases\n",
    "#if OR>1, the response variable becomes more likely as the explanatory one increases\n",
    "print('Odds Ratios')\n",
    "print(np.exp(lreg1.params))\n",
    "#Study subjects in the treatment group (cond=1) are 0.88 times as likely to have had an arrest since release on parole\n",
    "#as study subjects in the control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# odd ratios with 95% confidence intervals\n",
    "params = lreg1.params\n",
    "conf = lreg1.conf_int()\n",
    "conf['OR'] = params\n",
    "conf.columns = ['Lower CI', 'Upper CI', 'OR']\n",
    "print (np.exp(conf))\n",
    "#we have 95% confidence that the population odds ratio will be between 0.57 and 1.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Machine learning encompasses a wide range of statistical methods. These can be used for:\n",
    "#1. Describe associations\n",
    "#2. Search for patterns\n",
    "#3. Make predictions\n",
    "#We typically do not use ML with hypotheses in mind. instead we learn from the data\n",
    "#We learn from the test set.\n",
    "#Accuracy = test error rate. The rate at which it correctly classifies or estimates.\n",
    "#Goal is to minimize test error rate.\n",
    "#Linear regression: accuracy = mean squared error\n",
    "#Variance = change in parameter estimates across different data sets\n",
    "#Bias = how far off model estimated values are from true values\n",
    "#ideally we want low variance and low bias, but they are negatively associated. As one decreases, the other increases.\n",
    "#Generally, complexity of model leads to high variance and low bias\n",
    "#Simple models will have lower variance, but also be more biased.\n",
    "#Logistic regression: accuracy = how well the model classifies observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Supervised Prediction includes:\n",
    "#Linear regression\n",
    "#Pattern recognition\n",
    "#Discriminant analysis\n",
    "#Multivariate function estimation\n",
    "#Supervised ML techniques\n",
    "#Decision trees\n",
    "#Like linear regression, decision trees are designed for supervised prediction problems.\n",
    "#Root node, and terminal nodes or leaves.\n",
    "#Growing the tree process: binary splits maximize correct classification; all cut points are tested; subgroups showing\n",
    "#similar outcomes are generated\n",
    "#Validating the tree: cross validation guards against overfit. A random subset is tested and only 'branches' that improve\n",
    "#the classification are retained\n",
    "#Selected sub tree is the lowest probability of misclassification\n",
    "#Trees allow the handling of many variables that cannot be done as efficiently in linear regression. They can also uncover\n",
    "#constellations of variables that can predict high or low rates of the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Strengths of decision trees\n",
    "#Can select from a large number of variables those and their interactions that are most important in determining the\n",
    "#target or response variable to be explained\n",
    "#They are easy to interpret and visualize, especially when the tree is small\n",
    "#Can handle large data sets well and can predict both binary, categorical target variables and also quantitative ones\n",
    "#Limitations: small changes in the data can lead to different splits and this can undermine the interpretability of the model\n",
    "#and decision trees are not very reproducible on future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reduced_dataset_clean = reduced_dataset.dropna()\n",
    "len(reduced_dataset_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors = reduced_dataset_clean.ix[:, reduced_dataset_clean.columns != 'allcrimes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "targets = reduced_dataset_clean['allcrimes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, targets, test_size= 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(pred_train.shape, pred_test.shape, tar_train.shape, tar_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "classifier = classifier.fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\u001c",
    "sklearn.metrics.confusion_matrix(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Random forests\n",
    "#Forests of trees\n",
    "#Splits on only ONE variable in each node. Variable with largest association with Target among\n",
    "#candidate variables. Only among variables randomly selected to be tested for that node.\n",
    "#First a subset of explanatory variables is selected at random\n",
    "#Next the node is split with the Best variable of the subset. After this node is split, a new list of subset variables\n",
    "#is selected at random to split on the next node.\n",
    "#typical k fold values: 5 or 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(n_estimators=25)\n",
    "classifier2 = classifier.fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions2 = classifier2.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#fit an extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(pred_train, tar_train)\n",
    "#display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trees = range(25)\n",
    "accuracy = np.zeros(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(trees)):\n",
    "    classifier = RandomForestClassifier(n_estimators = idx + 1)\n",
    "    classifier = classifier.fit(pred_train, tar_train)\n",
    "    predictions = classifier.predict(pred_test)\n",
    "    accuracy[idx] = sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.plot(trees, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lasso regression\n",
    "#penalized regression method\n",
    "#supervised learning method\n",
    "#shrinkage and selection method\n",
    "#shrinkage = constraints on parameters that shrinks coefficients to 0\n",
    "#selection = identifies most imp. variables associated with response variable\n",
    "#Can increase prediction accuracy and improve model interpretability vs standard OLS\n",
    "#When lambda=0, it becomes OLS regression\n",
    "#Bias increases and variance decreases as lambda increases\n",
    "#in Lasso regression, penalty is not fair if variables are not on the same scale\n",
    "#standardize all predictor variables to have means equal to 0 and sd = 1\n",
    "#Lasso regression has several algorithms, among them LAR (least angle regression-)\n",
    "#sklearn library refers to the penalty term as 'alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Limitations of lasso regression\n",
    "#1. Selection of variables is 100% statistically driven\n",
    "#2. If predictors are correlated, lasso arbitrarily selects one\n",
    "#3. Estimating p values is not straightforward\n",
    "#4. Different selection methods or statistical softwares can provide different results\n",
    "#5. No guarantee that selected model is not overfitted nor that it's the best model\n",
    "#All regression models can produce meaningless models without human intervention\n",
    "#Best approach is a combination of ML, human intervention, and independent application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictors['cond'] = preprocessing.scale(predictors['cond'].astype('float64'))\n",
    "predictors['sex'] = preprocessing.scale(predictors['sex'].astype('float64'))\n",
    "predictors['age'] = preprocessing.scale(predictors['age'].astype('float64'))\n",
    "predictors['living_situation'] = preprocessing.scale(predictors['living_situation'].astype('float64'))\n",
    "predictors['support'] = preprocessing.scale(predictors['support'].astype('float64'))\n",
    "predictors['allarrests'] = preprocessing.scale(predictors['allarrests'].astype('float64'))\n",
    "predictors['anyarrest'] = preprocessing.scale(predictors['anyarrest'].astype('float64'))\n",
    "predictors['alldrugs'] = preprocessing.scale(predictors['alldrugs'].astype('float64'))\n",
    "predictors['anydrugs'] = preprocessing.scale(predictors['anydrugs'].astype('float64'))\n",
    "predictors['anycrime'] = preprocessing.scale(predictors['anycrime'].astype('float64'))\n",
    "predictors['arrest_9mo'] = preprocessing.scale(predictors['arrest_9mo'].astype('float64'))\n",
    "predictors['reincarc_9mo'] = preprocessing.scale(predictors['reincarc_9mo'].astype('float64'))\n",
    "predictors['num_arrest'] = preprocessing.scale(predictors['num_arrest'].astype('float64'))\n",
    "predictors['num_reincarc'] = preprocessing.scale(predictors['num_reincarc'].astype('float64'))\n",
    "predictors['violent_charge'] = preprocessing.scale(predictors['violent_charge'].astype('float64'))\n",
    "predictors['property_charge'] = preprocessing.scale(predictors['property_charge'].astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#split data into train and test tests\n",
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, targets, \n",
    "                                                              test_size=.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# specify the lasso regression model\n",
    "model=LassoLarsCV(cv=10, precompute=False).fit(pred_train,tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print variable names and regression coefficients\n",
    "dict(zip(predictors.columns, model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot coefficient progression\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "ax = plt.gca()\n",
    "plt.plot(m_log_alphas, model.coef_path_.T)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha CV')\n",
    "plt.ylabel('Regression Coefficients')\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.title('Regression Coefficients Progression for Lasso Paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot mean square error for each fold\n",
    "m_log_alphascv = -np.log10(model.cv_alphas_)\n",
    "plt.figure()\n",
    "plt.plot(m_log_alphascv, model.cv_mse_path_, ':')\n",
    "plt.plot(m_log_alphascv, model.cv_mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha CV')\n",
    "plt.legend()\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Mean squared error on each fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# MSE from training and test data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
    "test_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
    "print ('training data MSE')\n",
    "print(train_error)\n",
    "print ('test data MSE')\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# R-square from training and test data\n",
    "rsquared_train=model.score(pred_train,tar_train)\n",
    "rsquared_test=model.score(pred_test,tar_test)\n",
    "print ('training data R-square')\n",
    "print(rsquared_train)\n",
    "print ('test data R-square')\n",
    "print(rsquared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cluster analysis\n",
    "#Unsupervised learning method = no response variable included in the analysis\n",
    "#Goal: to have less variance within clusters, and more between clusters\n",
    "#Can also be used as a method of data reduction, to reduce number of variables\n",
    "#to the number of categorical variables equal to the clusters produced\n",
    "#Canonical discriminant analysis:\n",
    "#creates a smaller number of variables\n",
    "#linear combinations of clustering variables\n",
    "#canonical variables are ordered by proportion of variance accounted for\n",
    "#majority of variance is accounted for by first few canonical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clustervar = predictors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "clus_train, clus_test = train_test_split(clustervar, test_size=.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# k-means cluster analysis for 1-9 clusters                                                           \n",
    "from scipy.spatial.distance import cdist\n",
    "clusters=range(1,10)\n",
    "meandist=[]\n",
    "\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(clus_train)\n",
    "    clusassign=model.predict(clus_train)\n",
    "    meandist.append(sum(np.min(cdist(clus_train, model.cluster_centers_, 'euclidean'), axis=1)) \n",
    "    / clus_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot average distance from observations from the cluster centroid\n",
    "to use the Elbow Method to identify number of clusters to choose\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(clusters, meandist)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Interpret 3 cluster solution\n",
    "model3=KMeans(n_clusters=3)\n",
    "model3.fit(clus_train)\n",
    "clusassign=model3.predict(clus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot clusters\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_2 = PCA(2)\n",
    "plot_columns = pca_2.fit_transform(clus_train)\n",
    "plt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=model3.labels_,)\n",
    "plt.xlabel('Canonical variable 1')\n",
    "plt.ylabel('Canonical variable 2')\n",
    "plt.title('Scatterplot of Canonical Variables for 3 Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BEGIN multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\"\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster training data to merge with the cluster assignment variable\n",
    "clus_train.reset_index(level=0, drop=True)\n",
    "# create a list that has the new index variable\n",
    "cluslist=list(clus_train['index'])\n",
    "# create a list of cluster assignments\n",
    "labels=list(model3.labels_)\n",
    "# combine index variable list with cluster assignment list into a dictionary\n",
    "newlist=dict(zip(cluslist, labels))\n",
    "newlist\n",
    "# convert newlist dictionary to a dataframe\n",
    "newclus=pd.DataFrame.from_dict(newlist, orient= 'index')\n",
    "newclus\n",
    "# rename the cluster assignment column\n",
    "newclus.columns = ['cluster']\n",
    "\n",
    "# now do the same for the cluster assignment variable\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster assignment dataframe \n",
    "# to merge with cluster training data\n",
    "newclus.reset_index(level=0, drop=True)\n",
    "# merge the cluster assignment dataframe with the cluster training variable dataframe\n",
    "# by the index variable\n",
    "merged_train=pd.merge(clus_train, newclus, left_index=True, right_index=True)\n",
    "merged_train.head(n=100)\n",
    "# cluster frequencies\n",
    "merged_train.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "END multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\"\n",
    "\n",
    "# FINALLY calculate clustering variable means by cluster\n",
    "clustergrp = merged_train.groupby('cluster').mean()\n",
    "print (\"Clustering variable means by cluster\")\n",
    "print(clustergrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# validate clusters in training data by examining cluster differences in GPA using ANOVA\n",
    "# first have to merge GPA with clustering variables and cluster assignment data \n",
    "allarrests_data['targets'] = pd.DataFrame(targets)\n",
    "# split GPA data into train and test sets\n",
    "arrests_train, arrests_test = train_test_split(allarrests_data, test_size=.3, random_state=123)\n",
    "arrests_train1=pd.DataFrame(arrests_train)\n",
    "arrests_train1.reset_index(level=0, inplace=True)\n",
    "merged_train_all=pd.merge(arrests_train1, merged_train, left_index=True, right_index=True)\n",
    "sub1 = merged_train_all[['targets', 'cluster']].dropna()\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi \n",
    "\n",
    "allarrests_mod = smf.ols(formula='targets ~ C(cluster)', data=sub1).fit()\n",
    "print (allarrests_mod.summary())\n",
    "\n",
    "print ('means for all arrests by cluster')\n",
    "m1= sub1.groupby('cluster').mean()\n",
    "print (m1)\n",
    "\n",
    "print ('standard deviations for all arrests by cluster')\n",
    "m2= sub1.groupby('cluster').std()\n",
    "print (m2)\n",
    "\n",
    "mc1 = multi.MultiComparison(sub1['targets'], sub1['cluster'])\n",
    "res1 = mc1.tukeyhsd()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
